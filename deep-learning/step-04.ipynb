{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MaxMinNormalization(x):\n",
    "    \"\"\"\n",
    "    线性归一化，将输入list归一化\n",
    "    :param x: list类型\n",
    "    :return: 归一化list\n",
    "    \"\"\"\n",
    "    x = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "    return x\n",
    "\n",
    "\n",
    "def normalization(list):\n",
    "    \"\"\"\n",
    "    归一化接口，目前只支持线性归一化\n",
    "    :param list: 矩阵形式\n",
    "    :return: 归一化矩阵\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for x in list:\n",
    "        out.append(MaxMinNormalization(x))\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_max(list):\n",
    "    \"\"\"\n",
    "    提取音频序列中的极大值特征\n",
    "    :param list，宽度固定为20维，长度不限\n",
    "    :return:20维数组\n",
    "    \"\"\"\n",
    "    average = []\n",
    "    arr_temp = np.array(list)\n",
    "    # arr_temp=np.dot(arr_temp,arr_temp.T)\n",
    "    for a in arr_temp:\n",
    "        average.append(max(a))\n",
    "    # average.append(math.atan(max(a)) * 2 / 3.1415926)\n",
    "    return average\n",
    "\n",
    "def load(file):\n",
    "    \"\"\"\n",
    "    输入文件名，加载数据\n",
    "    :param file:文件名\n",
    "    :return:浮点型数组\n",
    "    \"\"\"\n",
    "    list = []\n",
    "    f = open(file, 'r', encoding='UTF-8')\n",
    "    for line in f:\n",
    "        line_list = line.replace(',\\n', '').split(',')\n",
    "        for i in range(len(line_list)):\n",
    "            line_list[i] = float(line_list[i])\n",
    "        list.append(line_list)\n",
    "        list = normalization(list)\n",
    "    return np.array(list)\n",
    "\n",
    "\n",
    "def load_y(file):\n",
    "    \"\"\"\n",
    "    输入文件名，加载数据\n",
    "    :param file:文件名\n",
    "    :return:浮点型数组\n",
    "    \"\"\"\n",
    "    list = []\n",
    "    f = open(file, 'r', encoding='UTF-8')\n",
    "    for line in f:\n",
    "        list.append(np.int64(line))\n",
    "    return np.array(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364, 20)\n",
      "(364,)\n",
      "(91, 20)\n",
      "(91,)\n",
      "[0.         1.         0.7439373  0.81523204 0.68245398 0.63583235\n",
      " 0.56919546 0.63004358 0.51317874 0.5784048  0.56526871 0.54378003\n",
      " 0.53581913 0.51616541 0.51459717 0.52425463 0.49956383 0.50344389\n",
      " 0.51078665 0.51413449]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X = load('data/x.txt')\n",
    "y = load('data/y.txt')\n",
    "\n",
    "new_y = []\n",
    "for item in y:\n",
    "    new_y.append(np.argmax(item))\n",
    "    \n",
    "y = np.array(new_y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(X[-1, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(273, 20)\n",
      "(273,)\n",
      "(91, 20)\n",
      "(91,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y                \n",
    "        out = x * y\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "\n",
    "        return dx, dy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine 层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W =W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 权重和偏置参数的导数\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 对应张量\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 还原输入数据的形状（对应张量）\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None # softmax的输出\n",
    "        self.t = None # 监督数据\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 监督数据是one-hot-vector的情况\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 溢出对策\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 监督数据是one-hot-vector的情况下，转换为正确解标签的索引\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 初始化权重\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 生成层\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x:输入数据, t:监督数据\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:输入数据, t:监督数据\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 设定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t ==>0.358974358974359   0.26373626373626374 \n",
      "5000\t ==>0.8058608058608059   0.7582417582417582 \n",
      "10000\t ==>0.8571428571428571   0.7582417582417582 \n",
      "15000\t ==>0.8791208791208791   0.8021978021978022 \n",
      "20000\t ==>0.8754578754578755   0.8131868131868132 \n",
      "25000\t ==>0.9304029304029304   0.8791208791208791 \n",
      "30000\t ==>0.9487179487179487   0.9010989010989011 \n",
      "35000\t ==>0.967032967032967   0.9010989010989011 \n",
      "40000\t ==>0.978021978021978   0.9120879120879121 \n",
      "45000\t ==>0.978021978021978   0.8901098901098901 \n",
      "50000\t ==>0.978021978021978   0.9010989010989011 \n",
      "55000\t ==>0.9816849816849816   0.9120879120879121 \n",
      "60000\t ==>0.9816849816849816   0.9010989010989011 \n",
      "65000\t ==>0.9853479853479854   0.9010989010989011 \n",
      "70000\t ==>0.989010989010989   0.9010989010989011 \n",
      "75000\t ==>0.9963369963369964   0.9010989010989011 \n",
      "80000\t ==>1.0   0.9120879120879121 \n",
      "85000\t ==>0.9963369963369964   0.9010989010989011 \n",
      "90000\t ==>0.9853479853479854   0.9010989010989011 \n",
      "95000\t ==>0.9963369963369964   0.9010989010989011 \n",
      "100000\t ==>1.0   0.9120879120879121 \n",
      "105000\t ==>1.0   0.9010989010989011 \n",
      "110000\t ==>1.0   0.9120879120879121 \n",
      "115000\t ==>1.0   0.9120879120879121 \n",
      "120000\t ==>0.9963369963369964   0.9120879120879121 \n",
      "125000\t ==>1.0   0.9120879120879121 \n",
      "130000\t ==>1.0   0.9120879120879121 \n",
      "135000\t ==>1.0   0.9010989010989011 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "network = TwoLayerNet(input_size=20, hidden_size=200, output_size=3)\n",
    "\n",
    "iters_num = 140000\n",
    "train_size = X_train.shape[0]\n",
    "batch_size = 50\n",
    "learning_rate = 0.04\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 5000)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    X_batch = X_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "    \n",
    "    # 梯度\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(X_batch, y_batch)\n",
    "    \n",
    "    # 更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(X_batch, y_batch)\n",
    "\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(X_train, y_train)\n",
    "        test_acc = network.accuracy(X_test, y_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        train_loss_list.append(loss)\n",
    "        print('{}\\t ==> train_acc:{}   test_acc:{}  loss:{}'.format(i, train_acc, test_acc, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12a6a0fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_loss_list))\n",
    "plt.plot(x, train_loss_list, label='train acc')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9120879120879121"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.accuracy(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/.local/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "# /Users/mac/Desktop/nwcd/客户资料/001蓝拓扑/test_data/h/110.mp3\n",
    "# y, sr = librosa.load('./data/test.mp3')\n",
    "y, sr = librosa.load('/Users/mac/Desktop/nwcd/客户资料/001蓝拓扑/test_data/w/0.mp3')\n",
    "a = librosa.feature.mfcc(y=y, sr=sr)\n",
    "predict_input = np.array(get_max(a)) \n",
    "\n",
    "predict_input = predict_input.reshape(1, 20)\n",
    "print(predict_input.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364, 20)\n",
      "(1, 20)\n",
      "(365, 20)\n",
      "[[-3.8404647   9.2484415  -5.46349251]]\n",
      "x:  h\n"
     ]
    }
   ],
   "source": [
    "def _load(file):\n",
    "    \"\"\"\n",
    "    输入文件名，加载数据\n",
    "    :param file:文件名\n",
    "    :return:浮点型数组\n",
    "    \"\"\"\n",
    "    list = []\n",
    "    f = open(file, 'r', encoding='UTF-8')\n",
    "    for line in f:\n",
    "        line_list = line.replace(',\\n', '').split(',')\n",
    "        for i in range(len(line_list)):\n",
    "            line_list[i] = float(line_list[i])\n",
    "        list.append(line_list)\n",
    "    return np.array(list)\n",
    " \n",
    "# 重新正则化\n",
    "_X = _load('data/x.txt')\n",
    "print(_X.shape)\n",
    "print(predict_input.shape)\n",
    "_X = np.concatenate( (_X, predict_input) )\n",
    "print(_X.shape)\n",
    "\n",
    "_X = normalization(_X)\n",
    "\n",
    "# 预测最后一个新加的输入参数\n",
    "network.predict(X[-1:,])\n",
    "real_lable =['z', 'h', 'w']\n",
    "print(network.predict(X[-1:,]))\n",
    "print(\"x: \", real_lable[np.argmax(network.predict(X[-1:,])[0])])\n",
    "# print(real_lable[np.argmax()]\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
