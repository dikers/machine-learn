{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词\n",
    "# 词语 -> id\n",
    "#   matrix -> [|V|, embed_size]\n",
    "#   词语A -> id(5)\n",
    "#   词表\n",
    "\n",
    "# label -> id\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import jieba # pip install jieba\n",
    "\n",
    "base_dir = './data/cnews/'\n",
    "\n",
    "# input files\n",
    "train_file = base_dir + 'cnews.train.txt'\n",
    "val_file = base_dir + 'cnews.val.txt'\n",
    "test_file = base_dir + 'cnews.test.txt'\n",
    "\n",
    "# output files\n",
    "seg_train_file = base_dir + 'cnews.train.seg.txt'\n",
    "seg_val_file = base_dir + 'cnews.val.seg.txt'\n",
    "seg_test_file = base_dir + 'cnews.test.seg.txt'\n",
    "\n",
    "vocab_file = base_dir + 'cnews.vocab.txt'\n",
    "category_file = base_dir + 'cnews.category.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/examples/github/machine-learn/deep-learning\n",
      "文件已经存在\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "if not os.path.exists(train_file):\n",
    "    print('{}不存在, 开始下载文件'.format(train_file))\n",
    "    !wget http://dikers.nwcd.s3-website-us-east-1.amazonaws.com/data-set/cnews_data.zip\n",
    "    !unzip ./cnews_data.zip\n",
    "    !rm ./cnews_data.zip\n",
    "    !mkdir ./data/cnews \n",
    "    !mv cnews.train.txt ./data/cnews/\n",
    "    !mv cnews.test.txt ./data/cnews/\n",
    "    !mv cnews.val.txt ./data/cnews/\n",
    "else:\n",
    "    print('文件已经存在')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用jieba分词\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 体育\n",
      "黄蜂vs湖人首发：科比带伤战保罗 加索尔救赎之战 新浪体育讯北京时间4月27日，NBA季后赛首轮洛杉矶湖人主场迎战新奥尔良黄蜂，此前的比赛中，双方战成2-2平，因此本场比赛对于两支球队来说都非常重要，赛前双方也公布了首发阵容：湖人队：费舍尔、科比、阿泰斯特、加索尔、拜纳姆黄蜂队：保罗、贝里内利、阿里扎、兰德里、奥卡福[新浪NBA官方微博][新浪NBA湖人新闻动态微博][新浪NBA专题][黄蜂vs湖人图文直播室](新浪体育)\n",
      "黄蜂/ vs/ 湖人/ 首发/ ：/ 科比/ 带伤/ 战/ 保罗/  / 加索尔/ 救赎/ 之战/  / 新浪/ 体育讯/ 北京/ 时间/ 4/ 月/ 27/ 日/ ，/ NBA/ 季后赛/ 首轮/ 洛杉矶/ 湖人/ 主场/ 迎战/ 新奥尔良/ 黄蜂/ ，/ 此前/ 的/ 比赛/ 中/ ，/ 双方/ 战成/ 2/ -/ 2/ 平/ ，/ 因此/ 本场/ 比赛/ 对于/ 两支/ 球队/ 来说/ 都/ 非常/ 重要/ ，/ 赛前/ 双方/ 也/ 公布/ 了/ 首发/ 阵容/ ：/ 湖人队/ ：/ 费舍尔/ 、/ 科比/ 、/ 阿泰斯特/ 、/ 加索尔/ 、/ 拜纳姆/ 黄蜂队/ ：/ 保罗/ 、/ 贝里/ 内利/ 、/ 阿里/ 扎/ 、/ 兰德/ 里/ 、/ 奥卡福/ [/ 新浪/ NBA/ 官方/ 微博/ ]/ [/ 新浪/ NBA/ 湖人/ 新闻动态/ 微博/ ]/ [/ 新浪/ NBA/ 专题/ ]/ [/ 黄蜂/ vs/ 湖人/ 图文/ 直播室/ ]/ (/ 新浪/ 体育/ )\n"
     ]
    }
   ],
   "source": [
    "with open(val_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "label, content = lines[0].strip('\\r\\n').split('\\t')\n",
    "word_iter = jieba.cut(content)\n",
    "\n",
    "print('label', label)\n",
    "print(content)\n",
    "print('/ '.join(word_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  将样本文件分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seg_file(input_file, output_seg_file):\n",
    "    \"\"\"Segment the sentences in each line in input_file\"\"\"\n",
    "    with open(input_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    with open(output_seg_file, 'w') as f:\n",
    "        for line in lines:\n",
    "            label, content = line.strip('\\r\\n').split('\\t')\n",
    "            word_iter = jieba.cut(content)\n",
    "            word_content = ''\n",
    "            for word in word_iter:\n",
    "                word = word.strip(' ')\n",
    "                if word != '':\n",
    "                    word_content += word + ' '\n",
    "            out_line = '%s\\t%s\\n' % (label, word_content.strip(' '))\n",
    "            f.write(out_line)\n",
    "        print('{} 文件分割完成, 输出路径{} .'.format(input_file, output_seg_file))\n",
    "        \n",
    "if not os.path.exists(seg_train_file):\n",
    "    generate_seg_file(train_file, seg_train_file)\n",
    "if not os.path.exists(seg_val_file):\n",
    "    generate_seg_file(val_file, seg_val_file)\n",
    "if not os.path.exists(seg_test_file):\n",
    "    generate_seg_file(test_file, seg_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocab_file(input_seg_file, output_vocab_file):\n",
    "    with open(input_seg_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    word_dict = {}\n",
    "    for line in lines:\n",
    "        label, content = line.strip('\\r\\n').split('\\t')\n",
    "        for word in content.split():\n",
    "            word_dict.setdefault(word, 0)\n",
    "            word_dict[word] += 1\n",
    "    # [(word, frequency), ..., ()]\n",
    "    sorted_word_dict = sorted(\n",
    "        word_dict.items(), key = lambda d:d[1], reverse=True)\n",
    "    with open(output_vocab_file, 'w') as f:\n",
    "        f.write('<UNK>\\t10000000\\n')\n",
    "        for item in sorted_word_dict:\n",
    "            f.write('%s\\t%d\\n' % (item[0], item[1]))\n",
    "\n",
    "generate_vocab_file(seg_train_file, vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab file 格式： \n",
    "词语 和 出现的数量\n",
    "\n",
    "```\n",
    "生活\t13141\n",
    "能够\t12911\n",
    "不会\t12898\n",
    "不同\t12871\n",
    "获得\t12870\n",
    "城市\t12825\n",
    "学校\t12775\n",
    "一定\t12736\n",
    "一直\t12606\n",
    "上海\t12574\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 对应的label 标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "体育\t5000\n",
      "娱乐\t5000\n",
      "家居\t5000\n",
      "房产\t5000\n",
      "教育\t5000\n",
      "时尚\t5000\n",
      "时政\t5000\n",
      "游戏\t5000\n",
      "科技\t5000\n",
      "财经\t5000\n"
     ]
    }
   ],
   "source": [
    "def generate_category_dict(input_file, category_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    category_dict = {}\n",
    "    for line in lines:\n",
    "        label, content = line.strip('\\r\\n').split('\\t')\n",
    "        category_dict.setdefault(label, 0)\n",
    "        category_dict[label] += 1\n",
    "    category_number = len(category_dict)\n",
    "    with open(category_file, 'w') as f:\n",
    "        for category in category_dict:\n",
    "            line = '%s\\n' % category\n",
    "            print('%s\\t%d' % (category, category_dict[category]))\n",
    "            f.write(line)\n",
    "            \n",
    "generate_category_dict(train_file, category_file)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 77323\n",
      "num_classes: 10\n",
      "label: 时尚, id: 5\n"
     ]
    }
   ],
   "source": [
    "num_word_threshold = 10\n",
    "num_timesteps = 50\n",
    "class Vocab:\n",
    "    def __init__(self, filename, num_word_threshold):\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._num_word_threshold = num_word_threshold\n",
    "        self._read_dict(filename)\n",
    "    \n",
    "    def _read_dict(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, frequency = line.strip('\\r\\n').split('\\t')\n",
    "            frequency = int(frequency)\n",
    "            if frequency < self._num_word_threshold:\n",
    "                continue\n",
    "            idx = len(self._word_to_id)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            self._word_to_id[word] = idx\n",
    "    \n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self._unk)\n",
    "    \n",
    "    def get_word_dict(self):\n",
    "        return self._word_to_id\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "    \n",
    "    def sentence_to_id(self, sentence):\n",
    "        word_ids = [self.word_to_id(cur_word) \\\n",
    "                    for cur_word in sentence.split()]\n",
    "        return word_ids\n",
    "\n",
    "\n",
    "class CategoryDict:\n",
    "    def __init__(self, filename):\n",
    "        self._category_to_id = {}\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            category = line.strip('\\r\\n')\n",
    "            idx = len(self._category_to_id)\n",
    "            self._category_to_id[category] = idx\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._category_to_id)\n",
    "        \n",
    "    def category_to_id(self, category):\n",
    "        if not category in self._category_to_id:\n",
    "            raise Execption(\n",
    "                \"%s is not in our category list\" % category_name)\n",
    "        return self._category_to_id[category]\n",
    "        \n",
    "vocab = Vocab(vocab_file, num_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "print('vocab_size: %d' % vocab_size)\n",
    "\n",
    "category_vocab = CategoryDict(category_file)\n",
    "num_classes = category_vocab.size()\n",
    "print('num_classes: %d' % num_classes)\n",
    "test_str = '时尚'\n",
    "print(\n",
    "    'label: %s, id: %d' % (\n",
    "        test_str,\n",
    "        category_vocab.category_to_id(test_str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成词的概率分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "total_count 77323\n",
      "0.03590134888713578\n",
      "0.9120665058006028\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "word_counts = vocab.get_word_dict()\n",
    "threshold = 1e-5\n",
    "print(word_counts['的'])\n",
    "total_count = len(word_counts)\n",
    "print('total_count', total_count)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "print(freqs['儿童'])\n",
    "# p_drop = {word: 1 - np.sqrt(threshold/(freqs[word] + 1e-10)) for word in word_counts}\n",
    "p_drop = {vocab.word_to_id(word): 1 - np.sqrt(threshold/(freqs[word] + 1e-10)) for word in word_counts}\n",
    "print(p_drop[100]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from %s /home/ec2-user/examples/github/machine-learn/deep-learning/data/cnews/cnews.train.seg.txt\n",
      "Loading data from %s /home/ec2-user/examples/github/machine-learn/deep-learning/data/cnews/cnews.val.seg.txt\n",
      "Loading data from %s /home/ec2-user/examples/github/machine-learn/deep-learning/data/cnews/cnews.test.seg.txt\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "class TextDataSet:\n",
    "    def __init__(self, filename, vocab, category_vocab, num_timesteps):\n",
    "        self._vocab = vocab\n",
    "        self._category_vocab = category_vocab\n",
    "        self._num_timesteps = num_timesteps\n",
    "        # matrix\n",
    "        self._inputs = []\n",
    "        # vector\n",
    "        self._outputs = []\n",
    "        self._indicator = 0\n",
    "        self._parse_file(filename)\n",
    "    \n",
    "    def _parse_file(self, filename):\n",
    "        print('Loading data from %s', filename)\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            label, content = line.strip('\\r\\n').split('\\t')\n",
    "            id_label = self._category_vocab.category_to_id(label)\n",
    "\n",
    "            id_words = self._vocab.sentence_to_id(content)\n",
    "            # 过滤掉一部分概率比较低的值\n",
    "#             print(len(content) , len(id_words))\n",
    "            id_words = [word for word in id_words if random.random() > (1 - p_drop[word])]\n",
    "\n",
    "            id_words = id_words[0: self._num_timesteps]\n",
    "            padding_num = self._num_timesteps - len(id_words)\n",
    "            id_words = id_words + [\n",
    "                self._vocab.unk for i in range(padding_num)]\n",
    "            self._inputs.append(id_words)\n",
    "            self._outputs.append(id_label)\n",
    "        self._inputs = np.asarray(self._inputs, dtype = np.int32)\n",
    "        self._outputs = np.asarray(self._outputs, dtype = np.int32)\n",
    "        self._random_shuffle()\n",
    "        self._num_examples = len(self._inputs)\n",
    "    \n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(len(self._inputs))\n",
    "        self._inputs = self._inputs[p]\n",
    "        self._outputs = self._outputs[p]\n",
    "    \n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            raise Execption(\"batch_size: %d is too large\" % batch_size)\n",
    "        \n",
    "        batch_inputs = self._inputs[self._indicator: end_indicator]\n",
    "        batch_outputs = self._outputs[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_inputs, batch_outputs\n",
    "    \n",
    "    def get_data(self):\n",
    "        batch_inputs = self._inputs\n",
    "        batch_outputs = self._outputs\n",
    "        return batch_inputs, batch_outputs\n",
    "        \n",
    "            \n",
    "train_dataset = TextDataSet(\n",
    "    seg_train_file, vocab, category_vocab,num_timesteps) \n",
    "val_dataset = TextDataSet(\n",
    "    seg_val_file, vocab, category_vocab, num_timesteps)\n",
    "test_dataset = TextDataSet(\n",
    "    seg_test_file, vocab, category_vocab, num_timesteps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "5000\n",
      "10000\n",
      "(50000, 50)\n",
      "(5000, 50)\n",
      "(10000, 50)\n",
      "y_train.shape (50000, 10)\n",
      "[[ 7792   807 54269  1854 37546 46534    24 29822   250    16 18430    11\n",
      "    326  6684    91   678    25    12   187   326   738 36285  1059  3025\n",
      "      1  5899   104   376  8049     2   596     2   175  6995 16208    20\n",
      "  10920   192   175  3632     6   192   175 21609     6   707   582  3632\n",
      "   6652   102]\n",
      " [  225   172    15 14489 12331    14 18850  2776  5834  4086   225    13\n",
      "   4225  2776   129  4086   649  1525   115   744    58   173  3340  2776\n",
      "   1132 14489 12331    79  6852    90    84  8565     3     4   225 72108\n",
      "   9210     2 68457   218   115 14023   165   173   174    34   211   433\n",
      "   2776   165]\n",
      " [30474   739  6123   849   422    17   250   428 30474 10889     2  6123\n",
      "    849   422    64  2289     5  1408 13679     3  1882 13590     5   608\n",
      "    821     1  6550   324  3262  3744     5  2540     2 33690 27637    20\n",
      "   3869   804   946   122 11748  5771     2  5182    32  1408   191   739\n",
      "  64589   474]]\n",
      "<class '__main__.Vocab'>\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "print(train_dataset.num_examples())\n",
    "print(val_dataset.num_examples())\n",
    "print(test_dataset.num_examples())\n",
    "\n",
    "x_train , y_train = train_dataset.get_data()\n",
    "x_val, y_val = val_dataset.get_data()\n",
    "x_test, y_test  = test_dataset.get_data()\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "print(x_train[5:8])\n",
    "print(type(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 32)            2474336   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                16010     \n",
      "=================================================================\n",
      "Total params: 2,490,346\n",
      "Trainable params: 2,490,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 17s 344us/step - loss: 0.9331 - acc: 0.7688 - val_loss: 0.4047 - val_acc: 0.8900\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 17s 343us/step - loss: 0.1766 - acc: 0.9519 - val_loss: 0.2448 - val_acc: 0.9288\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 17s 339us/step - loss: 0.1058 - acc: 0.9695 - val_loss: 0.2020 - val_acc: 0.9392\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 17s 347us/step - loss: 0.0777 - acc: 0.9780 - val_loss: 0.1943 - val_acc: 0.9426\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 18s 353us/step - loss: 0.0602 - acc: 0.9831 - val_loss: 0.1928 - val_acc: 0.9444\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 17s 340us/step - loss: 0.0471 - acc: 0.9867 - val_loss: 0.1897 - val_acc: 0.9450\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 17s 340us/step - loss: 0.0364 - acc: 0.9904 - val_loss: 0.1902 - val_acc: 0.9458\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 17s 337us/step - loss: 0.0290 - acc: 0.9920 - val_loss: 0.1951 - val_acc: 0.9460\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 17s 342us/step - loss: 0.0222 - acc: 0.9938 - val_loss: 0.2007 - val_acc: 0.9470\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 17s 338us/step - loss: 0.0173 - acc: 0.9955 - val_loss: 0.2038 - val_acc: 0.9466\n",
      "10000/10000 [==============================] - 0s 21us/step\n",
      "[0.24261047044694423, 0.9409]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "maxlen = x_test.shape[1]\n",
    "_epochs = 10\n",
    "model = Sequential()\n",
    "# We specify the maximum input length to our Embedding layer\n",
    "# so we can later flatten the embedded inputs\n",
    "model.add(Embedding(vocab_size, 32, input_length=maxlen))\n",
    "# After the Embedding layer, \n",
    "# our activations have shape `(samples, maxlen, 8)`.\n",
    "\n",
    "# We flatten the 3D tensor of embeddings \n",
    "# into a 2D tensor of shape `(samples, maxlen * 8)`\n",
    "model.add(Flatten())\n",
    "# We add the classifier on top\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=_epochs,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "print(model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "_epochs = 10\n",
    "epochs = range(1, _epochs+1)\n",
    "val_loss = history.history['val_loss']\n",
    "plt.plot(epochs, val_loss, 'b+', color='r',label='Model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=maxlen))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=_epochs,\n",
    "                    batch_size=64,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
