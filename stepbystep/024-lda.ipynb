{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本主题可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyldavis jieba -i  https://mirrors.163.com/pypi/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import jieba # pip install jieba\n",
    "import urllib.request as ur\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_url = 'https://dikers-data.s3.cn-northwest-1.amazonaws.com.cn/dataset/cnews_data.zip'\n",
    "base_dir = './datasets/cnews/'\n",
    "\n",
    "# input files\n",
    "val_file = base_dir + 'cnews.val.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/examples/machine-learning/stepbystep\n",
      "./dataset/cnews/cnews.val.txt不存在, 开始下载文件\n",
      "Archive:  ./cnews_data.zip\n",
      "  inflating: cnews.test.txt          \n",
      "   creating: __MACOSX/\n",
      "  inflating: __MACOSX/._cnews.test.txt  \n",
      "  inflating: cnews.val.txt           \n",
      "  inflating: __MACOSX/._cnews.val.txt  \n",
      "  inflating: cnews.train.txt         \n",
      "  inflating: __MACOSX/._cnews.train.txt  \n",
      "mkdir: cannot create directory ‘./datasets/cnews’: File exists\n",
      "CPU times: user 251 ms, sys: 273 ms, total: 523 ms\n",
      "Wall time: 3.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!pwd\n",
    "if not os.path.exists(val_file):\n",
    "    !mkdir ./datasets\n",
    "    !mkdir ./datasets/cnews\n",
    "    print('{}不存在, 开始下载文件'.format(val_file))\n",
    "    ur.urlretrieve(data_file_url, \"cnews_data.zip\")\n",
    "    !unzip ./cnews_data.zip\n",
    "    !rm ./cnews_data.zip\n",
    "    !mkdir ./datasets/cnews \n",
    "    !mv cnews.train.txt ./datasets/cnews/\n",
    "    !mv cnews.test.txt ./datasets/cnews/\n",
    "    !mv cnews.val.txt ./datasets/cnews/\n",
    "    !rm -fr __MACOSX\n",
    "else:\n",
    "    print('文件已经存在')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stopwordslist(stopwords_file):\n",
    "    return [line.strip() for line in open(stopwords_file ,encoding='UTF-8').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data(stopwords, val_file, out_file):\n",
    "    train_data = []\n",
    "\n",
    "\n",
    "    with open(val_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        index = 0\n",
    "        for index, line in enumerate(lines):\n",
    "            label, content = line.strip('\\r\\n').split('\\t')\n",
    "            word_iter = jieba.cut(content)\n",
    "            new_word_iter = []\n",
    "            for w in word_iter:\n",
    "                if w not in stopwords and w != ' ':\n",
    "                    new_word_iter.append(w)\n",
    "            train_data.append(' '.join(new_word_iter))\n",
    "\n",
    "\n",
    "    with open(out_file, 'w') as f:\n",
    "        for i in train_data:\n",
    "            f.write(i.strip()+'\\n')\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(train_data_file):\n",
    "    train_data = []\n",
    "    with open(train_data_file, 'r', encoding='utf-8') as f:\n",
    "\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            train_data.append(line.replace('\\n', ''))\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_file = './sample_data/stopwords.txt'\n",
    "stopwords = stopwordslist(stopwords_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = './datasets/train_data.txt'\n",
    "train_data = create_train_data(stopwords, val_file, out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_train_data('./datasets/train_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字典长度： 20340\n",
      "Topic #1:\n",
      "设计 开发商 社区 论坛 位于 信息 号楼 地图搜索 相册 样板间 均价 平方米 开盘 项目 户型\n",
      "Topic #2:\n",
      "造型 最佳 奥斯卡 上映 电影节 明星 黑色 娱乐 性感 导演 时尚 导语 影片 组图 搭配\n",
      "Topic #3:\n",
      "球员 房价 行业 股票 中国 经济 湖人 球队 分红 热火 房地产 公司 投资 比赛 市场\n",
      "Topic #4:\n",
      "机身 光学 发展 企业 支持 消费者 佳能 采用 中国 拍摄 产品 像素 功能 活动 游戏\n",
      "Topic #5:\n",
      "题目 作文 学生 单词 文章 大学 听力 答案 阅读 六级 信息 四级 英语 考生 四六级\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=5)\n",
    "tfidf_mat = tfidf_vectorizer.fit_transform(train_data)\n",
    "\n",
    "print('字典长度：', len(tfidf_vectorizer.vocabulary_))\n",
    "\n",
    "n_topics = 5      # 自定义主题个数\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, batch_size=8, random_state=0)\n",
    "# 使用TF-IDF矩阵拟合LDA模型\n",
    "lda_model.fit(tfidf_mat)\n",
    "\n",
    "\n",
    "# 主题词打印函数\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\"%(topic_idx+1))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[-n_top_words-1:-1]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  打印每个主题前15个关键字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 15\n",
    "tf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(lda_model, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   数据可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pyLDAvis.sklearn.prepare(lda_model, tfidf_mat, tfidf_vectorizer)\n",
    "pyLDAvis.show(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:amazonei_tensorflow_p36]",
   "language": "python",
   "name": "conda-env-amazonei_tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
