{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN LSTM 示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下载文件并且解压\n",
    "\n",
    "```\n",
    "wget https://dikers-data.s3.cn-northwest-1.amazonaws.com.cn/dataset/simple-examples.tgz\n",
    "tar  xvf simple-examples.tgz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/simple-examples/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将文件根据句末分割符 <eos> 来分割\n",
    "def read_words(filename):\n",
    "    with tf.gfile.GFile(filename, 'r') as f:\n",
    "        return f.read().replace('\\n', '<eos>').split()\n",
    "    \n",
    "    \n",
    "# 构造从单词到唯一整数值的映射\n",
    "# 后面的其他数的整数值按照它们在数据集里出现的次数多少来排序，出现较多的排前面\n",
    "# 单词 the 出现频次最多，对应整数值是 0\n",
    "# <unk> 表示 unknown（未知），第二多，整数值为 1\n",
    "def build_vocab(filename):\n",
    "    data = read_words(filename)\n",
    "\n",
    "    # 用 Counter 统计单词出现的次数，为了之后按单词出现次数的多少来排序\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "\n",
    "    # 单词到整数的映射\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "    return word_to_id\n",
    "\n",
    "# 将文件里的单词都替换成独一的整数\n",
    "def file_to_word_ids(filename, word_to_id):\n",
    "    data = read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "train_data length: 929589\n",
      "valid_data length: 73760\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    train_path = os.path.join(data_path, 'ptb.train.txt')\n",
    "    valid_path = os.path.join(data_path, 'ptb.valid.txt')\n",
    "    test_path = os.path.join(data_path, 'ptb.test.txt')\n",
    "    \n",
    "    # 建立词汇表，将所有单词（word）转为唯一对应的整数值（id）\n",
    "    word_to_id = build_vocab(train_path)\n",
    "\n",
    "    # 训练，验证和测试数据\n",
    "    train_data = file_to_word_ids(train_path, word_to_id)\n",
    "    valid_data = file_to_word_ids(valid_path, word_to_id)\n",
    "    test_data = file_to_word_ids(test_path, word_to_id)\n",
    "\n",
    "    # 所有不重复单词的个数\n",
    "    vocab_size = len(word_to_id)\n",
    "\n",
    "    # 反转一个词汇表：为了之后从 整数 转为 单词\n",
    "    id_to_word = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "\n",
    "    \n",
    "#     print(word_to_id)\n",
    "#     print(\"===================\")\n",
    "    print(vocab_size)\n",
    "#     print(\"===================\")\n",
    "#     print(train_data[:10])\n",
    "#     print(\"===================\")\n",
    "#     print(\" \".join([id_to_word[x] for x in train_data[:10]]))\n",
    "#     print(\"===================\")\n",
    "    print(\"train_data length: {}\".format(len(train_data)))\n",
    "    print(\"valid_data length: {}\".format(len(valid_data)))\n",
    "#     return train_data, valid_data, test_data, vocab_size, id_to_word\n",
    "\n",
    "load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, input_obj, is_training, hidden_size, \n",
    "                 vocab_size, num_layers, dropout=0.5, init_scale=0.05):\n",
    "        self.is_training = is_training\n",
    "        self.input_obj = input_obj\n",
    "        self.batch_size = input_obj.batch_size\n",
    "        self.num_steps = input_obj.num_steps\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        with tf.device('/cpu:0'):\n",
    "            # 创建 词向量（Word Embedding），Embedding 表示 Dense Vector（密集向量）\n",
    "            # 词向量本质上是一种单词聚类（Clustering）的方法\n",
    "            embedding = tf.Variable(tf.random_uniform([vocab_size, self.hidden_size], -init_scale, init_scale))\n",
    "            # embedding_lookup 返回词向量\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self.input_obj.input_data)\n",
    "            \n",
    "            if is_training and dropout < 1:\n",
    "                inputs = tf.nn.dropout(inputs, dropout)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
