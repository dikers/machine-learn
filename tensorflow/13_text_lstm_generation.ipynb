{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf    version: 2.0.0\n",
      "keras version: 2.2.4-tf\n",
      "matplotlib 3.0.3\n",
      "numpy 1.16.4\n",
      "pandas 0.24.2\n",
      "sklearn 0.21.2\n",
      "tensorflow 2.0.0\n",
      "tensorflow_core.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print('tf    version: {}'.format(tf.__version__) )\n",
    "print('keras version: {}'.format(keras.__version__) )\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filepath = './data/shakespeare.txt'\n",
    "\n",
    "text = open(input_filepath, 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "char2idx = {char:idx for idx, char in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 146 ms, sys: 12.4 ms, total: 159 ms\n",
      "Wall time: 157 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n",
      "[18 47 56 57 58  1 15 47 58 47]\n",
      "First Citi\n"
     ]
    }
   ],
   "source": [
    "print(len(text_as_int))\n",
    "\n",
    "print(text_as_int[:10])\n",
    "print(text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.2 ms, sys: 114 ms, total: 173 ms\n",
      "Wall time: 190 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def split_input_target(id_text):\n",
    "    \"\"\"\n",
    "    abcde -> abcd, bcde, 输入和输出\n",
    "    \"\"\"\n",
    "    return id_text[0:-1], id_text[1:]\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "seq_length = 100\n",
    "\n",
    "seq_dataset = char_dataset.batch(seq_length +1 , drop_remainder=True)\n",
    "\n",
    "\n",
    "\n",
    "# for ch_id in char_dataset.take(2):\n",
    "#     print(ch_id, idx2char[ch_id.numpy()])\n",
    "    \n",
    "# for seq_id in seq_dataset.take(2):\n",
    "#     print(seq_id)\n",
    "#     print(repr(''.join(idx2char[seq_id.numpy()])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function split_input_target at 0x7f0437ae6b70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function split_input_target at 0x7f0437ae6b70>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "WARNING: Entity <function split_input_target at 0x7f0437ae6b70> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <function split_input_target at 0x7f0437ae6b70>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59]\n",
      "[47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43  1\n",
      " 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43 39\n",
      " 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49  6\n",
      "  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0\n",
      " 37 53 59  1]\n",
      "[39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1\n",
      " 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0\n",
      " 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8\n",
      "  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1\n",
      " 63 53 59  1]\n",
      "[56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1 58\n",
      " 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0 13\n",
      " 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8  0\n",
      "  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1 63\n",
      " 53 59  1 49]\n"
     ]
    }
   ],
   "source": [
    "seq_dataset = seq_dataset.map(split_input_target)\n",
    "\n",
    "for item_input, item_output in seq_dataset.take(2):\n",
    "    print(item_input.numpy())\n",
    "    print(item_output.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "buffer_size = 10000\n",
    "\n",
    "seq_dataset = seq_dataset.shuffle(buffer_size).batch(\n",
    "    batch_size, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                              batch_input_shape=[batch_size,None]), \n",
    "        keras.layers.LSTM(units=rnn_units, \n",
    "                          stateful =True,\n",
    "                          recurrent_initializer='glorot_uniform', \n",
    "                          return_sequences=True), \n",
    "        keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "    \n",
    "model = build_model(\n",
    "    vocab_size = vocab_size,\n",
    "    embedding_dim = embedding_dim,\n",
    "    rnn_units = rnn_units,\n",
    "    batch_size= batch_size\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in seq_dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  'noble Paris and true Romeo dead.\\nShe wakes; and I entreated her come forth,\\nAnd bear this work of he'\n",
      "\n",
      "Output:  'oble Paris and true Romeo dead.\\nShe wakes; and I entreated her come forth,\\nAnd bear this work of hea'\n",
      "\n",
      "Predictions:  \"Su!c-e:x?;zBkt'Wj$VKFjBAC.&rAN:ftfNBeYa,yoPRA.BDprYfjhn&EErI J!ZKzI:PCa$.aC;.-ubnjDYo:Yp!I\\nmV iG&sOF\"\n"
     ]
    }
   ],
   "source": [
    "# random sampling 随机策略\n",
    "# greedy 贪心策略\n",
    "\n",
    "sample_indices = tf.random.categorical(logits=example_batch_predictions[0], \n",
    "                     num_samples=1)\n",
    "# print(sample_indices)\n",
    "sample_indices = tf.squeeze(sample_indices, axis=1)\n",
    "# print(sample_indices.numpy())\n",
    "print(\"Input: \", repr(''.join(idx2char[input_example_batch[0]])) )\n",
    "print('')\n",
    "print(\"Output: \", repr(''.join(idx2char[target_example_batch[0]])) )\n",
    "print('')\n",
    "print(\"Predictions: \", repr(''.join(idx2char[sample_indices])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100)\n",
      "4.1749544\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return keras.losses.sparse_categorical_crossentropy(\n",
    "        labels, logits, from_logits=True)\n",
    " \n",
    "model.compile(optimizer= 'adam', loss=loss)\n",
    "example_loss = loss(target_example_batch, example_batch_predictions)\n",
    "\n",
    "print(example_loss.shape)\n",
    "print(example_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "172/172 [==============================] - 9s 53ms/step - loss: 2.5889\n",
      "Epoch 2/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 1.8782\n",
      "Epoch 3/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 1.6320\n",
      "Epoch 4/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 1.5009\n",
      "Epoch 5/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 1.4215\n",
      "Epoch 6/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 1.3672\n",
      "Epoch 7/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 1.3224\n",
      "Epoch 8/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 1.2835\n",
      "Epoch 9/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 1.2491\n",
      "Epoch 10/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 1.2141\n",
      "Epoch 11/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 1.1800\n",
      "Epoch 12/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 1.1442\n",
      "Epoch 13/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 1.1085\n",
      "Epoch 14/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 1.0704\n",
      "Epoch 15/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 1.0309\n",
      "Epoch 16/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.9899\n",
      "Epoch 17/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.9505\n",
      "Epoch 18/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.9107\n",
      "Epoch 19/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.8717\n",
      "Epoch 20/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.8326\n",
      "Epoch 21/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.7970\n",
      "Epoch 22/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.7625\n",
      "Epoch 23/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.7317\n",
      "Epoch 24/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.7020\n",
      "Epoch 25/50\n",
      "172/172 [==============================] - 7s 41ms/step - loss: 0.6739\n",
      "Epoch 26/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.6516\n",
      "Epoch 27/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.6267\n",
      "Epoch 28/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.6082\n",
      "Epoch 29/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.5894\n",
      "Epoch 30/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.5714\n",
      "Epoch 31/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.5571\n",
      "Epoch 32/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.5428\n",
      "Epoch 33/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.5306\n",
      "Epoch 34/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.5185\n",
      "Epoch 35/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.5099\n",
      "Epoch 36/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.5012\n",
      "Epoch 37/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.4923\n",
      "Epoch 38/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.4845\n",
      "Epoch 39/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.4768\n",
      "Epoch 40/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.4722\n",
      "Epoch 41/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.4635\n",
      "Epoch 42/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.4595\n",
      "Epoch 43/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.4529\n",
      "Epoch 44/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.4512\n",
      "Epoch 45/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.4457\n",
      "Epoch 46/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.4423\n",
      "Epoch 47/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.4374\n",
      "Epoch 48/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.4354\n",
      "Epoch 49/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.4320\n",
      "Epoch 50/50\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.4297\n"
     ]
    }
   ],
   "source": [
    "output_dir = './data/text_generation'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "checkpoint_prefix = os.path.join(output_dir, 'ckpt_{epoch}')\n",
    "\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_prefix,\n",
    "    save_weights_only = True)\n",
    "\n",
    "epochs = 50\n",
    "history = model.fit(seq_dataset, epochs = epochs,\n",
    "                    callbacks = [checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/text_generation/ckpt_50'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: yet we will help you to-day?\n",
      "\n",
      "CHRISTOPHER:\n",
      "So sadly aftly repent the sin the adverse perchant;\n",
      "And the new-made be deposed; ven Edward's son;\n",
      "Thou shalt not live to speak; and happileting to\n",
      "father:\n",
      "From my hap monst to do, and so are yet\n",
      "Be said it may be abhorror!\n",
      "And, as become of one foul generation, when he lies,\n",
      "And liberal toe!n if they can behold him.\n",
      "\n",
      "ROMEO:\n",
      "\n",
      "SICINIUS:\n",
      "Let me hear not?\n",
      "O, my Lord Hastings are the all renowned of an addlcreath?\n",
      "\n",
      "Nurse:\n",
      "Marry, well, then.\n",
      "If I be not, he's head; as it were, in a wretch\n",
      "That return limit of them, good my lord.\n",
      "\n",
      "ANGELO:\n",
      "I would not tit the time; when we saw in this blood of it\n",
      "his venom these services to want his.\n",
      "\n",
      "RICH:\n",
      "The silence oftheir life fairly of my father's life.\n",
      "\n",
      "JOHN OF GAUNT:\n",
      "God's!\n",
      "\n",
      "Third Gentleman:\n",
      "No: the princess hear no less upon the head,\n",
      "That covenants subjects now to bear\n",
      "My sovereign mischief; measure he be agreed;\n",
      "O, what consperve a gloss terrorant\n",
      "In soltier ubstrain'd fast: he may come mother.\n",
      "Hark, hark!\n"
     ]
    }
   ],
   "source": [
    "model2 = build_model(vocab_size, embedding_dim, \n",
    "                    rnn_units, \n",
    "                    batch_size=1)\n",
    "\n",
    "model2.load_weights(tf.train.latest_checkpoint(output_dir))\n",
    "model2.build(tf.TensorShape([1, None]))\n",
    "\n",
    "# start ch sequence A, \n",
    "# A -> model -> b\n",
    "# A.append(b) -> B\n",
    "# B -> model -> c\n",
    "# B.append(c) -> C\n",
    "# C -> model -> d\n",
    "\n",
    "def generate_text(model, start_string, num_generate=1000):\n",
    "    input_eval = [char2idx[ch] for ch in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    text_generated = []\n",
    "    model.reset_states()\n",
    "    \n",
    "    for _ in range(num_generate):\n",
    "        #1. model inference -> predictions\n",
    "        #2. sample -> ch -> text_generated.\n",
    "        #3. update input_eval\n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predicted_id = tf.random.categorical(\n",
    "        predictions, num_samples= 1)[-1, 0].numpy()\n",
    "        \n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "    return start_string + ''.join(text_generated)\n",
    "        \n",
    "new_text = generate_text(model2, \"All: \")\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
