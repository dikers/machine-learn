{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM 实现 \n",
    "\n",
    "(https://zhuanlan.zhihu.com/p/81549798)[https://zhuanlan.zhihu.com/p/81549798]\n",
    "\n",
    "(https://zhuanlan.zhihu.com/p/32085405)[https://zhuanlan.zhihu.com/p/32085405]\n",
    "\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/54868269\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version:  2.1.0\n",
      "GPU :  False\n",
      "GPU list []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.keras import activations\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "print('tf version: ', tf.__version__)\n",
    "print('GPU : ', tf.test.is_gpu_available())\n",
    "print('GPU list', tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_CELL(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=256, **kwargs):\n",
    "        # lstm 维度\n",
    "        self.units = units\n",
    "        super(LSTM_CELL, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        self.w = self.add_weight(shape=(input_dim, self.units * 4), name='kernel',\n",
    "            initializer=initializers.get('glorot_uniform'))\n",
    "        print(\"LSTM w.shape: {}\".format(self.w.shape))\n",
    "        \n",
    "\n",
    "        # u 保存hadden 的权重\n",
    "        self.u = self.add_weight(shape=(self.units, self.units * 4),\n",
    "                                                name='recurrent_kernel',\n",
    "                                                initializer=initializers.get('orthogonal'))\n",
    "        print(\"LSTM u.shape: {}\".format(self.u.shape))\n",
    "        self.bias = self.add_weight(\n",
    "            shape=(self.units * 4), name='bias',\n",
    "            initializer=initializers.get('zeros'))\n",
    "        print(\"LSTM b.shape: {}\".format(self.bias.shape))\n",
    "#         self.recurrent_activation = activations.get('hard_sigmoid')\n",
    "#         self.activation = activations.get('tanh')\n",
    "        \n",
    "        self.sigmoid = activations.get('hard_sigmoid')\n",
    "        self.tanh = activations.get('tanh')\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        \"\"\"\n",
    "        input shape 是三维 ，同时会计算batch_size 个样本数据\n",
    "        \"\"\"\n",
    "#         print(\"---- call ---  states \", states)\n",
    "        last_h = states[0]   # h(t-1)\n",
    "        last_c = states[1]   # c(c-1)\n",
    "        # i information 输入门\n",
    "        # f forget 遗忘门\n",
    "        # o output 输出门\n",
    "        # c. cell\n",
    "        # 四个权重保持在一个变量里面\n",
    "        w_i, w_f, w_c, w_o = tf.split(self.w, num_or_size_splits=4, axis=1)\n",
    "#         print('w_i.shape', w_i.shape)\n",
    "        b_i, b_f, b_c, b_o = tf.split(self.bias, num_or_size_splits=4, axis=0)\n",
    "#         print('b_i.shape', b_i.shape)\n",
    "        # w x\n",
    "        x_i = K.dot(inputs, w_i)\n",
    "#         print(\"inputs shape {} * w_i shape {} = x_i shape {} \".format(inputs.shape, w_i.shape, x_i.shape))\n",
    "        x_f = K.dot(inputs, w_f)\n",
    "        x_c = K.dot(inputs, w_c)\n",
    "        x_o = K.dot(inputs, w_o)\n",
    "        # w x + b\n",
    "        x_i = K.bias_add(x_i, b_i)\n",
    "        x_f = K.bias_add(x_f, b_f)\n",
    "        x_c = K.bias_add(x_c, b_c)\n",
    "        x_o = K.bias_add(x_o, b_o)\n",
    "\n",
    "        u_i, u_f, u_c, u_o = tf.split(self.u, num_or_size_splits=4, axis=1)\n",
    "        # w x + u * h + x\n",
    "        i = self.sigmoid(x_i + K.dot(last_h, u_i))\n",
    "        f = self.sigmoid(x_f + K.dot(last_h, u_f))\n",
    "#         c = f * last_c + self.tanh(x_c + K.dot(last_h, u_c))\n",
    "        c = f * last_c + i * self.tanh(x_c + K.dot(last_h, u_c))\n",
    "\n",
    "        o = self.sigmoid(x_o + K.dot(last_h, u_o))\n",
    "\n",
    "        # 计算 h\n",
    "        h = o * self.tanh(c)\n",
    "        \n",
    "        return h, (h, c)\n",
    "\n",
    "class Rnn(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=128):\n",
    "        super(Rnn, self).__init__()\n",
    "        self.cell = LSTM_CELL(units)\n",
    "        self.init_state = None\n",
    "    def build(self, input_shape):\n",
    "        print('Rnn shape: ', input_shape)\n",
    "        shape = input_shape.as_list()\n",
    "        n_batch = shape[0]\n",
    "        init_h = tf.zeros(shape=[n_batch, self.cell.units])\n",
    "        init_c = init_h\n",
    "        self.init_state = (init_h, init_c)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        前向传播， 依次遍历每个时间序列\n",
    "        第一个维度是 样本数量\n",
    "        第二个维度是 时间序列\n",
    "        \"\"\"\n",
    "        # time step\n",
    "        ts = inputs.shape.as_list()[1]\n",
    "#       print(inputs.shape.as_list())\n",
    "        h, c = self.init_state\n",
    "        for i in range(ts):\n",
    "            h, (h, c) = self.cell(inputs[:, i],(h, c))\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 4)\n",
      "Rnn shape:  (2, 3, 4)\n",
      "LSTM w.shape: (4, 20)\n",
      "LSTM u.shape: (5, 20)\n",
      "LSTM b.shape: (20,)\n",
      "(2, 5)\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.normal(shape=(2, 3, 4))\n",
    "print(a.shape)\n",
    "rnn = Rnn(5)\n",
    "h = rnn(a)\n",
    "print(h.shape)\n",
    "# print(a[0])\n",
    "# [4, 28, 28]\n",
    "# LSTM w.shape: (28, 1024)\n",
    "# LSTM u.shape: (256, 1024)\n",
    "# LSTM b.shape: (1024,)\n",
    "# w_i.shape (28, 256)\n",
    "# b_i.shape (256,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        # office lstm\n",
    "        #self.rnn = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(256))\n",
    "        # my lstm\n",
    "        self.rnn = Rnn(256)\n",
    "        # 三层网络， 128 Dense + 10 softmax\n",
    "        self.d1 = tf.keras.layers.Dense(128, activation=\"relu\")\n",
    "        self.d2 = tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        前向传播， 预测， 输入x， 输出y。 \n",
    "        \"\"\"\n",
    "        x = self.rnn(x)\n",
    "        # [batch_size, d1.output_size], [4, 128]\n",
    "        x = self.d1(x)\n",
    "        # [batch_size, d2.output_size], [4, 10]\n",
    "        # 最后输出分类\n",
    "        x = self.d2(x)\n",
    "#       print('------x.shape', x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, loss, opti, images, labels, train_loss, train_acc):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # pred [batch_size, n_class] (4, 10)\n",
    "        pred = model(images)\n",
    "        loss_val = loss(labels, pred)\n",
    "    train_loss.update_state(loss_val)\n",
    "    train_acc.update_state(labels, pred)\n",
    "    grad = tape.gradient(loss_val, model.trainable_variables)\n",
    "    opti.apply_gradients(zip(grad, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化器\n",
    "opti = tf.keras.optimizers.Adam()\n",
    "# 定义损失函数\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "# 用于记录损失值\n",
    "train_loss = tf.keras.metrics.Mean()\n",
    "# 记录正确率\n",
    "train_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "# 加载数据\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), _ = fashion_mnist.load_data()\n",
    "train_images = train_images / 255.0\n",
    "num_used = 5000\n",
    "train_images = train_images[:num_used]\n",
    "train_labels = train_labels[:num_used]\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(10000).batch(4)\n",
    "# 定义模型\n",
    "model = MyModel()\n",
    "epochs = 30\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer my_model_21 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Rnn shape:  (4, 28, 28)\n",
      "LSTM w.shape: (28, 1024)\n",
      "LSTM u.shape: (256, 1024)\n",
      "LSTM b.shape: (1024,)\n",
      "Time: 27.05 s, Epoch:  0, loss: 1.07026, acc: 0.59680\n",
      "Time: 21.57 s, Epoch:  1, loss: 0.72783, acc: 0.72920\n",
      "Time: 21.61 s, Epoch:  2, loss: 0.58067, acc: 0.78720\n",
      "Time: 21.49 s, Epoch:  3, loss: 0.52058, acc: 0.80900\n",
      "Time: 21.50 s, Epoch:  4, loss: 0.47311, acc: 0.82900\n",
      "Time: 21.60 s, Epoch:  5, loss: 0.43479, acc: 0.83560\n",
      "Time: 21.55 s, Epoch:  6, loss: 0.41062, acc: 0.84780\n",
      "Time: 21.50 s, Epoch:  7, loss: 0.38830, acc: 0.85960\n",
      "Time: 21.66 s, Epoch:  8, loss: 0.36042, acc: 0.86460\n"
     ]
    }
   ],
   "source": [
    "list_time_cost = list()\n",
    "list_acc = list()\n",
    "for epoch in range(epochs):\n",
    "    # train\n",
    "    train_loss.reset_states()\n",
    "    train_acc.reset_states()\n",
    "    # images [batch_size, height, width] (4, 28, 28)\n",
    "    # labels [batch_size]\n",
    "    start = time.time()\n",
    "    for images, labels in train_ds:\n",
    "        train_step(model, loss, opti, images, labels, train_loss, train_acc)\n",
    "    ends = time.time()\n",
    "    cost = ends - start\n",
    "    list_time_cost.append(cost)\n",
    "    list_acc.append(train_acc.result().numpy())\n",
    "    print(\"Time: {:.2f} s, Epoch: {:2d}, loss: {:.5f}, acc: {:.5f}\".format(cost, epoch, train_loss.result(), train_acc.result()))\n",
    "# with open(\"./output/my_lstm_acc.pkl\", \"wb\") as fw:\n",
    "#     pickle.dump(list_acc, fw)\n",
    "# with open(\"./output/my_lstm_time_cost.pkl\", \"wb\") as fw:\n",
    "#     pickle.dump(list_time_cost, fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time: 23.789902687072754 s, Epoch: 0, loss: 1.1165646314620972, acc: 0.5685999989509583\n",
    "Time: 19.317150831222534 s, Epoch: 1, loss: 0.8056125044822693, acc: 0.6935999989509583\n",
    "Time: 19.2503023147583 s, Epoch: 2, loss: 0.6855613589286804, acc: 0.7441999912261963\n",
    "Time: 19.254036903381348 s, Epoch: 3, loss: 0.5773577690124512, acc: 0.7815999984741211\n",
    "Time: 19.334920406341553 s, Epoch: 4, loss: 0.5494149923324585, acc: 0.7972000241279602\n",
    "Time: 19.337989330291748 s, Epoch: 5, loss: 0.5215439200401306, acc: 0.8105999827384949\n",
    "Time: 19.4918429851532 s, Epoch: 6, loss: 0.47755488753318787, acc: 0.829200029373169\n",
    "Time: 19.27661943435669 s, Epoch: 7, loss: 0.4736003279685974, acc: 0.8271999955177307\n",
    "Time: 19.279857635498047 s, Epoch: 8, loss: 0.4204665422439575, acc: 0.843999981880188\n",
    "Time: 19.294177293777466 s, Epoch: 9, loss: 0.4283628761768341, acc: 0.8410000205039978\n",
    "Time: 19.24098014831543 s, Epoch: 10, loss: 0.39684203267097473, acc: 0.8497999906539917\n",
    "Time: 19.27296805381775 s, Epoch: 11, loss: 0.36575642228126526, acc: 0.8640000224113464\n",
    "Time: 19.261507272720337 s, Epoch: 12, loss: 0.4031131863594055, acc: 0.8593999743461609\n",
    "Time: 19.254382848739624 s, Epoch: 13, loss: 0.33121854066848755, acc: 0.8722000122070312\n",
    "Time: 19.271594047546387 s, Epoch: 14, loss: 0.314357727766037, acc: 0.8820000290870667\n",
    "Time: 19.265925645828247 s, Epoch: 15, loss: 0.3158433437347412, acc: 0.8841999769210815\n",
    "Time: 19.297107934951782 s, Epoch: 16, loss: 0.29900258779525757, acc: 0.8871999979019165\n",
    "Time: 19.178181886672974 s, Epoch: 17, loss: 0.2762836217880249, acc: 0.8960000276565552\n",
    "Time: 19.29625701904297 s, Epoch: 18, loss: 0.24783462285995483, acc: 0.9053999781608582\n",
    "Time: 19.205928802490234 s, Epoch: 19, loss: 0.2404624968767166, acc: 0.9110000133514404\n",
    "Time: 19.252617597579956 s, Epoch: 20, loss: 0.24850600957870483, acc: 0.9056000113487244\n",
    "Time: 19.25585174560547 s, Epoch: 21, loss: 0.2243300825357437, acc: 0.9151999950408936\n",
    "Time: 19.26186513900757 s, Epoch: 22, loss: 0.20446330308914185, acc: 0.9211999773979187\n",
    "Time: 19.31869077682495 s, Epoch: 23, loss: 0.19534704089164734, acc: 0.9272000193595886\n",
    "Time: 19.297197103500366 s, Epoch: 24, loss: 0.17534077167510986, acc: 0.9354000091552734\n",
    "Time: 19.262805223464966 s, Epoch: 25, loss: 0.17609521746635437, acc: 0.9330000281333923\n",
    "Time: 19.22975778579712 s, Epoch: 26, loss: 0.17506620287895203, acc: 0.9381999969482422\n",
    "Time: 19.275073051452637 s, Epoch: 27, loss: 0.15615135431289673, acc: 0.9409999847412109\n",
    "Time: 19.438047647476196 s, Epoch: 28, loss: 0.14412060379981995, acc: 0.9458000063896179\n",
    "Time: 19.246238946914673 s, Epoch: 29, loss: 0.1463460624217987, acc: 0.9458000063896179\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#c = f * last_c + self.tanh(x_c + K.dot(last_h, u_c))\n",
    "c = f * last_c + i * self.tanh(x_c + K.dot(last_h, u_c))\n",
    "```\n",
    " 修改完上面一处错误后， acc 提高。        \n",
    "        \n",
    "```\n",
    "Time: 26.709998607635498 s, Epoch: 0, loss: 1.152647614479065, acc: 0.5522000193595886\n",
    "Time: 21.37618327140808 s, Epoch: 1, loss: 0.7082579135894775, acc: 0.7386000156402588\n",
    "Time: 21.438363790512085 s, Epoch: 2, loss: 0.5829753875732422, acc: 0.7871999740600586\n",
    "Time: 21.412479400634766 s, Epoch: 3, loss: 0.5179308652877808, acc: 0.8044000267982483\n",
    "Time: 21.25425386428833 s, Epoch: 4, loss: 0.47378724813461304, acc: 0.8223999738693237\n",
    "Time: 21.292400360107422 s, Epoch: 5, loss: 0.43394234776496887, acc: 0.8407999873161316\n",
    "Time: 21.256401777267456 s, Epoch: 6, loss: 0.4198310077190399, acc: 0.8442000150680542\n",
    "Time: 21.24660563468933 s, Epoch: 7, loss: 0.3805636763572693, acc: 0.8565999865531921\n",
    "Time: 21.293148517608643 s, Epoch: 8, loss: 0.3567814826965332, acc: 0.871399998664856\n",
    "Time: 21.26962113380432 s, Epoch: 9, loss: 0.3456974923610687, acc: 0.873199999332428\n",
    "Time: 21.267067432403564 s, Epoch: 10, loss: 0.3175477981567383, acc: 0.8859999775886536\n",
    "Time: 21.255532026290894 s, Epoch: 11, loss: 0.3029397130012512, acc: 0.8889999985694885\n",
    "Time: 21.34101128578186 s, Epoch: 12, loss: 0.29142022132873535, acc: 0.8913999795913696\n",
    "Time: 21.273035526275635 s, Epoch: 13, loss: 0.26742011308670044, acc: 0.8949999809265137\n",
    "Time: 21.25704288482666 s, Epoch: 14, loss: 0.2575889527797699, acc: 0.9061999917030334\n",
    "Time: 21.324198246002197 s, Epoch: 15, loss: 0.2270210236310959, acc: 0.9147999882698059\n",
    "Time: 21.39011859893799 s, Epoch: 16, loss: 0.21490620076656342, acc: 0.9187999963760376\n",
    "Time: 21.248369216918945 s, Epoch: 17, loss: 0.21009555459022522, acc: 0.920799970626831\n",
    "Time: 21.258670806884766 s, Epoch: 18, loss: 0.19424863159656525, acc: 0.9246000051498413\n",
    "Time: 21.290088176727295 s, Epoch: 19, loss: 0.18225084245204926, acc: 0.928600013256073\n",
    "Time: 21.236432552337646 s, Epoch: 20, loss: 0.1635647863149643, acc: 0.9383999705314636\n",
    "Time: 21.3145968914032 s, Epoch: 21, loss: 0.15962575376033783, acc: 0.9408000111579895\n",
    "Time: 21.270111322402954 s, Epoch: 22, loss: 0.14077134430408478, acc: 0.9455999732017517\n",
    "Time: 21.28567624092102 s, Epoch: 23, loss: 0.1404210329055786, acc: 0.9477999806404114\n",
    "Time: 21.267570972442627 s, Epoch: 24, loss: 0.1189630851149559, acc: 0.9563999772071838\n",
    "Time: 21.305675745010376 s, Epoch: 25, loss: 0.1059480682015419, acc: 0.9598000049591064\n",
    "Time: 21.32499861717224 s, Epoch: 26, loss: 0.11065905541181564, acc: 0.9577999711036682\n",
    "Time: 21.27551817893982 s, Epoch: 27, loss: 0.10627779364585876, acc: 0.9613999724388123\n",
    "Time: 21.334386825561523 s, Epoch: 28, loss: 0.08813607692718506, acc: 0.9664000272750854\n",
    "Time: 21.29513669013977 s, Epoch: 29, loss: 0.07467116415500641, acc: 0.9742000102996826\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
