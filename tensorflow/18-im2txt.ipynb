{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pprint\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow import gfile\n",
    "from tensorflow import logging\n",
    "import _pickle as pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import glob\n",
    "import math\n",
    "from PIL import Image\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取字幕文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dir  = 'datastes/'\n",
    "output_dir = 'output/'\n",
    "if not gfile.Exists(datasets_dir):\n",
    "    gfile.MakeDirs(datasets_dir)\n",
    "    \n",
    "if not gfile.Exists(output_dir):\n",
    "    gfile.MakeDirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = datasets_dir + 'im2txt/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd \n",
    "os.chdir(datasets_dir)\n",
    "!pwd && ls \n",
    "\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Downloading...\")\n",
    "if not os.path.exists(base_dir):\n",
    "    call(\n",
    "        'wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI HAR Dataset.zip\"',\n",
    "        shell=True\n",
    "    )\n",
    "    print(\"Downloading done.\\n\")\n",
    "else:\n",
    "    print(\"Dataset already downloaded. Did not download twice.\\n\")\n",
    "\n",
    "\n",
    "print(\"Extracting...\")\n",
    "extract_directory = os.path.abspath(\"im2txt\")\n",
    "if not os.path.exists(extract_directory):\n",
    "    call(\n",
    "        'unzip -nq \"im2txt.zip\"',\n",
    "        shell=True\n",
    "    )\n",
    "    print(\"Extracting successfully done to {}.\".format(extract_directory))\n",
    "else:\n",
    "    print(\"Dataset already extracted. Did not extract twice.\\n\")\n",
    "\n",
    "\n",
    "!pwd ls\n",
    "os.chdir(\"..\")\n",
    "!pwd \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_file = base_dir  + 'Flickr8k.token.txt'\n",
    "output_vocab_file = base_dir + 'vocab.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "{1: 2,\n",
      " 2: 15,\n",
      " 3: 47,\n",
      " 4: 238,\n",
      " 5: 556,\n",
      " 6: 1316,\n",
      " 7: 2711,\n",
      " 8: 3445,\n",
      " 9: 3863,\n",
      " 10: 4307,\n",
      " 11: 4363,\n",
      " 12: 4079,\n",
      " 13: 3649,\n",
      " 14: 3049,\n",
      " 15: 2459,\n",
      " 16: 1791,\n",
      " 17: 1352,\n",
      " 18: 956,\n",
      " 19: 728,\n",
      " 20: 496,\n",
      " 21: 329,\n",
      " 22: 189,\n",
      " 23: 178,\n",
      " 24: 112,\n",
      " 25: 66,\n",
      " 26: 60,\n",
      " 27: 30,\n",
      " 28: 23,\n",
      " 29: 11,\n",
      " 30: 11,\n",
      " 31: 9,\n",
      " 32: 6,\n",
      " 33: 3,\n",
      " 34: 4,\n",
      " 35: 1,\n",
      " 38: 1}\n"
     ]
    }
   ],
   "source": [
    "def count_vocab(token_file):\n",
    "    with open(token_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    max_length_of_sentences = 0\n",
    "    length_dict = {}\n",
    "    vocab_dict = {}\n",
    "    \n",
    "    for line in lines: \n",
    "        image_id, description = line.strip('\\n').split('\\t')\n",
    "        words = description.strip(' ').split()\n",
    "        max_length_of_sentences = max(max_length_of_sentences, len(words))\n",
    "        length_dict.setdefault(len(words), 0)\n",
    "        length_dict[len(words)] += 1\n",
    "        \n",
    "        for word in words: \n",
    "            vocab_dict.setdefault(word, 0)\n",
    "            vocab_dict[word] +=1 \n",
    "            \n",
    "    print(max_length_of_sentences)\n",
    "    pprint.pprint(length_dict)\n",
    "    return vocab_dict\n",
    "\n",
    "vocab_dict = count_vocab(token_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vocab_dict = sorted(vocab_dict.items(), \n",
    "                          key = lambda d:d[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab 已经存在\n"
     ]
    }
   ],
   "source": [
    "if not gfile.Exists(output_vocab_file):\n",
    "    print(\"vocab 不存在， 新建 {} \".format(output_vocab_file))\n",
    "    with open(output_vocab_file, 'w') as f:\n",
    "        f.write(\"<UNK>\\t100000\\n\")\n",
    "        for item in sorted_vocab_dict:\n",
    "            f.write('%s\\t%d\\n' % item)\n",
    "else:\n",
    "    print(\"vocab 已经存在\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图片名称和描述读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = base_dir + 'weight/classify_image_graph_def.pb'\n",
    "input_description_file = token_file\n",
    "input_img_dir = base_dir + 'Flicker8k_Dataset'\n",
    "output_folder = \"output/im2txt/feature_extraction_inception_v3\"\n",
    "batch_size = 500\n",
    "input_img_feature_dir = output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:num of all images: 8091\n",
      "['1000268201_693b08cb0e.jpg',\n",
      " '1001773457_577c3a7d70.jpg',\n",
      " '1002674143_1b742ab4b8.jpg',\n",
      " '1003163366_44323f5815.jpg',\n",
      " '1007129816_e794419615.jpg',\n",
      " '1007320043_627395c3d8.jpg',\n",
      " '1009434119_febe49276a.jpg',\n",
      " '1012212859_01547e3f17.jpg',\n",
      " '1015118661_980735411b.jpg',\n",
      " '1015584366_dfcec3c85a.jpg']\n",
      "['A dog shakes its head near the shore , a red ball next to it .',\n",
      " 'A white dog shakes on the edge of a beach with an orange ball .',\n",
      " 'Dog with orange ball at feet , stands on shore shaking off water',\n",
      " 'White dog playing with a red ball on the shore near the water .',\n",
      " 'White dog with brown ears standing near water with head turned to one side .']\n"
     ]
    }
   ],
   "source": [
    "if not gfile.Exists(output_folder):\n",
    "    gfile.MakeDirs(output_folder)\n",
    "    \n",
    "def parse_token_file(token_file):\n",
    "    img_name_to_tokens = {}\n",
    "    \n",
    "    with gfile.GFile(token_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    for line in lines:\n",
    "        img_id, description = line.strip('\\r\\n').split('\\t')\n",
    "        img_name, _ = img_id.split('#')\n",
    "        img_name_to_tokens.setdefault(img_name, [])\n",
    "        img_name_to_tokens[img_name].append(description)\n",
    "    return img_name_to_tokens\n",
    "  \n",
    "img_name_to_tokens = parse_token_file(input_description_file)\n",
    "all_img_names = list(img_name_to_tokens.keys())\n",
    "logging.info(\"num of all images: %d\" % len(all_img_names))\n",
    "pprint.pprint(list(img_name_to_tokens.keys())[0:10])\n",
    "pprint.pprint(img_name_to_tokens['1012212859_01547e3f17.jpg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-b686e04d7d62>:2: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n"
     ]
    }
   ],
   "source": [
    "def load_pretrained_inception_v3(model_file):\n",
    "    with gfile.FastGFile(model_file, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        \n",
    "        _ = tf.import_graph_def(graph_def, name=\"\")\n",
    "        \n",
    "load_pretrained_inception_v3(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = int(len(all_img_names) / batch_size)\n",
    "if len(all_img_names) % batch_size != 0:\n",
    "    num_batches += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "print(num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将每张图片的特征提取出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No  0/17\t进度 1.000% INFO:tensorflow:\n",
      "writing to file output/im2txt/feature_extraction_inception_v3/image_features-0.pickle\n",
      " No  1/17\t进度 1.000% INFO:tensorflow:\n",
      "writing to file output/im2txt/feature_extraction_inception_v3/image_features-1.pickle\n",
      " No  2/17\t进度 1.000% INFO:tensorflow:\n",
      "writing to file output/im2txt/feature_extraction_inception_v3/image_features-2.pickle\n",
      " No  3/17\t进度 1.000% INFO:tensorflow:\n",
      "writing to file output/im2txt/feature_extraction_inception_v3/image_features-3.pickle\n",
      " No  4/17\t进度 1.000% INFO:tensorflow:\n",
      "writing to file output/im2txt/feature_extraction_inception_v3/image_features-4.pickle\n",
      " No  5/17\t进度 1.000% INFO:tensorflow:\n",
      "writing to file output/im2txt/feature_extraction_inception_v3/image_features-5.pickle\n",
      " No  6/17\t进度 1.000% INFO:tensorflow:\n",
      "writing to file output/im2txt/feature_extraction_inception_v3/image_features-6.pickle\n",
      " No  7/17\t进度 1.000% INFO:tensorflow:\n",
      "writing to file output/im2txt/feature_extraction_inception_v3/image_features-7.pickle\n",
      " No  8/17\t进度 1.000% INFO:tensorflow:\n",
      "writing to file output/im2txt/feature_extraction_inception_v3/image_features-8.pickle\n",
      " No  9/17\t进度 1.000% INFO:tensorflow:\n",
      "writing to file output/im2txt/feature_extraction_inception_v3/image_features-9.pickle\n",
      " No 10/17\t进度 1.000% INFO:tensorflow:\n",
      "writing to file output/im2txt/feature_extraction_inception_v3/image_features-10.pickle\n",
      " No 11/17\t进度 1.000% INFO:tensorflow:\n",
      "writing to file output/im2txt/feature_extraction_inception_v3/image_features-11.pickle\n",
      " No 12/17\t进度 1.000% INFO:tensorflow:\n",
      "writing to file output/im2txt/feature_extraction_inception_v3/image_features-12.pickle\n",
      " No 13/17\t进度 1.000% INFO:tensorflow:\n",
      "writing to file output/im2txt/feature_extraction_inception_v3/image_features-13.pickle\n",
      " No 14/17\t进度 1.000% INFO:tensorflow:\n",
      "writing to file output/im2txt/feature_extraction_inception_v3/image_features-14.pickle\n",
      " No 15/17\t进度 1.000% INFO:tensorflow:\n",
      "writing to file output/im2txt/feature_extraction_inception_v3/image_features-15.pickle\n",
      " No 16/17\t进度 1.000% INFO:tensorflow:\n",
      "writing to file output/im2txt/feature_extraction_inception_v3/image_features-16.pickle\n",
      "CPU times: user 1h 10min 42s, sys: 46.3 s, total: 1h 11min 28s\n",
      "Wall time: 13min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not gfile.Exists(output_folder + 'image_features-0.pickle'):\n",
    "    with tf.Session() as sess:\n",
    "        second_to_last_tensor = sess.graph.get_tensor_by_name(\"pool_3:0\")\n",
    "        for i in range(num_batches):\n",
    "            batch_img_names = all_img_names[i*batch_size: (i+1)*batch_size]\n",
    "            batch_features = []\n",
    "\n",
    "            index = 0 \n",
    "            for img_name in batch_img_names: \n",
    "                index += 1\n",
    "                img_path = os.path.join(input_img_dir, img_name)\n",
    "    #             logging.info(\"processing img %s\" % img_name)\n",
    "                progress =  100.0*index/len(batch_img_names)\n",
    "                print(\"\\r No{:3d}/{}  progress: {:.2f}% \"\n",
    "                      .format(i, num_batches, progress),end=\"\")\n",
    "                if not gfile.Exists(img_path):\n",
    "                    logging.warn(\"\\n%s doesn't exists\" % img_path)\n",
    "                    continue\n",
    "                img_data = gfile.FastGFile(img_path, \"rb\").read()\n",
    "                feature_vector = sess.run(second_to_last_tensor,\n",
    "                                          feed_dict = {\"DecodeJpeg/contents:0\": img_data})\n",
    "                batch_features.append(feature_vector)\n",
    "            batch_features = np.vstack(batch_features)\n",
    "            output_filename = os.path.join(output_folder, \"image_features-%d.pickle\" % i)\n",
    "            logging.info(\"\\nwriting to file %s\" % output_filename)\n",
    "            with gfile.GFile(output_filename, 'w') as f:\n",
    "                pickle.dump((batch_img_names, batch_features), f)\n",
    "else:\n",
    "    logging.info(\"图片特征已经提取完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img_feature_dit = output_folder\n",
    "input_vocab_file = output_vocab_file\n",
    "output_dir = './output/im2txt/local_run'\n",
    "\n",
    "if not gfile.Exists(output_dir):\n",
    "    gfile.MakeDirs(output_dir)\n",
    "    \n",
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        num_vocab_word_threshold = 3, \n",
    "        num_embedding_nodes  =32, \n",
    "        num_timesteps = 10, \n",
    "        num_lstm_nodes = [64, 64], \n",
    "        num_lstm_layers = 2, \n",
    "        num_fc_nodes = 32, \n",
    "        batch_size = 64, \n",
    "        cell_type = 'lstm', \n",
    "        clip_lstm_grads = 1.0, \n",
    "        learning_rate = 0.001, \n",
    "        keep_prob = 0.8, \n",
    "        log_frequent= 500,\n",
    "        save_frequent= 2000,\n",
    "    )\n",
    "hps = get_default_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('batch_size', 64), ('cell_type', 'lstm'), ('clip_lstm_grads', 1.0), ('keep_prob', 0.8), ('learning_rate', 0.001), ('log_frequent', 500), ('num_embedding_nodes', 32), ('num_fc_nodes', 32), ('num_lstm_layers', 2), ('num_lstm_nodes', [64, 64]), ('num_timesteps', 10), ('num_vocab_word_threshold', 3), ('save_frequent', 2000)]\n"
     ]
    }
   ],
   "source": [
    "print(hps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 封装词表的类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(self, filename, word_num_threshold):\n",
    "        self._id_to_word = {}\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._eos = -1\n",
    "        self._word_num_threshold = word_num_threshold\n",
    "        self._read_dict(filename)\n",
    "        \n",
    "    def _read_dict(self, filename):\n",
    "        with gfile.GFile(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, occurence = line.strip('\\r\\n').split('\\t')\n",
    "            occurence = int(occurence)\n",
    "            if word != '<UNK>' and occurence < self._word_num_threshold:\n",
    "                continue\n",
    "            idx = len(self._id_to_word)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            elif word == '.':\n",
    "                self._eos = idx\n",
    "            if idx in self._id_to_word or word in self._word_to_id:\n",
    "                raise Exception('duplicate words in vocab file')\n",
    "            self._word_to_id[word] = idx\n",
    "            self._id_to_word[idx] = word\n",
    "            \n",
    "            \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "\n",
    "    @property\n",
    "    def eos(self):\n",
    "        return self._eos\n",
    "\n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self.unk)\n",
    "\n",
    "    def id_to_word(self, cur_id):\n",
    "        return self._id_to_word.get(cur_id, '<UNK>')\n",
    "\n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        word_ids = [self.word_to_id(cur_word) for cur_word in sentence.split(' ')]\n",
    "        return word_ids\n",
    "\n",
    "    def decode(self, sentence_id):\n",
    "        words = [self.id_to_word(word_id) for word_id in sentence_id]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "def parse_token_file(token_file):\n",
    "    \"\"\"Parses token file.\"\"\"\n",
    "    img_name_to_tokens = {}\n",
    "    with gfile.GFile(token_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        img_id, description = line.strip('\\r\\n').split('\\t')\n",
    "        img_name, _ = img_id.split('#')\n",
    "        img_name_to_tokens.setdefault(img_name, [])\n",
    "        img_name_to_tokens[img_name].append(description)\n",
    "    return img_name_to_tokens\n",
    "\n",
    "def convert_token_to_id(img_name_to_tokens, vocab):\n",
    "    \"\"\"Converts tokens of each description of imgs to id. \"\"\"\n",
    "    img_name_to_token_ids = {}\n",
    "    for img_name in img_name_to_tokens:\n",
    "        img_name_to_token_ids.setdefault(img_name, [])\n",
    "        descriptions = img_name_to_tokens[img_name]\n",
    "        for description in descriptions:\n",
    "            token_ids = vocab.encode(description)\n",
    "            img_name_to_token_ids[img_name].append(token_ids)\n",
    "    return img_name_to_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:vocab_size: 4241\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(input_vocab_file, hps.num_vocab_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "logging.info(\"vocab_size: %d\" % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1720, 549, 1, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "print(vocab.encode('I have a dream .'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wearing white swinging playing high\n"
     ]
    }
   ],
   "source": [
    "print(vocab.decode([23, 14, 333, 35, 200 ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_to_tokens = parse_token_file(input_description_file)\n",
    "img_name_to_token_ids = convert_token_to_id(img_name_to_tokens, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:num of all images: 8091\n",
      "['1000268201_693b08cb0e.jpg',\n",
      " '1001773457_577c3a7d70.jpg',\n",
      " '1002674143_1b742ab4b8.jpg']\n",
      "['A black and white dog is running in a grassy garden surrounded by a white '\n",
      " 'fence .',\n",
      " 'A black and white dog is running through the grass .',\n",
      " 'A Boston terrier is running in the grass .',\n",
      " 'A Boston Terrier is running on lush green grass in front of a white fence .',\n",
      " 'A dog runs on the green grass near a wooden fence .']\n",
      "----------------\n",
      "INFO:tensorflow:num of all images: 8091\n",
      "['1000268201_693b08cb0e.jpg',\n",
      " '1001773457_577c3a7d70.jpg',\n",
      " '1002674143_1b742ab4b8.jpg']\n",
      "[[3, 15, 8, 14, 9, 7, 33, 4, 1, 127, 681, 278, 55, 1, 14, 179, 2],\n",
      " [3, 15, 8, 14, 9, 7, 33, 34, 5, 43, 2],\n",
      " [3, 2005, 1184, 7, 33, 4, 5, 43, 2],\n",
      " [3, 2005, 2171, 7, 33, 6, 2172, 56, 43, 4, 49, 12, 1, 14, 179, 2],\n",
      " [3, 9, 76, 6, 5, 56, 43, 68, 1, 199, 179, 2]]\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"num of all images: %d\" % len(img_name_to_tokens))\n",
    "pprint.pprint(list(img_name_to_tokens.keys())[0:3])\n",
    "pprint.pprint(img_name_to_tokens['1009434119_febe49276a.jpg'])\n",
    "print('----------------')\n",
    "logging.info(\"num of all images: %d\" % len(img_name_to_token_ids))\n",
    "pprint.pprint(list(img_name_to_token_ids.keys())[0:3])\n",
    "pprint.pprint(img_name_to_token_ids['1009434119_febe49276a.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['output/im2txt/feature_extraction_inception_v3/image_features-10.pickle',\n",
      " 'output/im2txt/feature_extraction_inception_v3/image_features-9.pickle',\n",
      " 'output/im2txt/feature_extraction_inception_v3/image_features-6.pickle',\n",
      " 'output/im2txt/feature_extraction_inception_v3/image_features-0.pickle',\n",
      " 'output/im2txt/feature_extraction_inception_v3/image_features-11.pickle',\n",
      " 'output/im2txt/feature_extraction_inception_v3/image_features-14.pickle',\n",
      " 'output/im2txt/feature_extraction_inception_v3/image_features-16.pickle',\n",
      " 'output/im2txt/feature_extraction_inception_v3/image_features-4.pickle',\n",
      " 'output/im2txt/feature_extraction_inception_v3/image_features-8.pickle',\n",
      " 'output/im2txt/feature_extraction_inception_v3/image_features-5.pickle',\n",
      " 'output/im2txt/feature_extraction_inception_v3/image_features-2.pickle',\n",
      " 'output/im2txt/feature_extraction_inception_v3/image_features-12.pickle',\n",
      " 'output/im2txt/feature_extraction_inception_v3/image_features-13.pickle',\n",
      " 'output/im2txt/feature_extraction_inception_v3/image_features-3.pickle',\n",
      " 'output/im2txt/feature_extraction_inception_v3/image_features-15.pickle',\n",
      " 'output/im2txt/feature_extraction_inception_v3/image_features-1.pickle',\n",
      " 'output/im2txt/feature_extraction_inception_v3/image_features-7.pickle']\n",
      "INFO:tensorflow:loading output/im2txt/feature_extraction_inception_v3/image_features-10.pickle\n",
      "INFO:tensorflow:loading output/im2txt/feature_extraction_inception_v3/image_features-9.pickle\n",
      "INFO:tensorflow:loading output/im2txt/feature_extraction_inception_v3/image_features-6.pickle\n",
      "INFO:tensorflow:loading output/im2txt/feature_extraction_inception_v3/image_features-0.pickle\n",
      "INFO:tensorflow:loading output/im2txt/feature_extraction_inception_v3/image_features-11.pickle\n",
      "INFO:tensorflow:loading output/im2txt/feature_extraction_inception_v3/image_features-14.pickle\n",
      "INFO:tensorflow:loading output/im2txt/feature_extraction_inception_v3/image_features-16.pickle\n",
      "INFO:tensorflow:loading output/im2txt/feature_extraction_inception_v3/image_features-4.pickle\n",
      "INFO:tensorflow:loading output/im2txt/feature_extraction_inception_v3/image_features-8.pickle\n",
      "INFO:tensorflow:loading output/im2txt/feature_extraction_inception_v3/image_features-5.pickle\n",
      "INFO:tensorflow:loading output/im2txt/feature_extraction_inception_v3/image_features-2.pickle\n",
      "INFO:tensorflow:loading output/im2txt/feature_extraction_inception_v3/image_features-12.pickle\n",
      "INFO:tensorflow:loading output/im2txt/feature_extraction_inception_v3/image_features-13.pickle\n",
      "INFO:tensorflow:loading output/im2txt/feature_extraction_inception_v3/image_features-3.pickle\n",
      "INFO:tensorflow:loading output/im2txt/feature_extraction_inception_v3/image_features-15.pickle\n",
      "INFO:tensorflow:loading output/im2txt/feature_extraction_inception_v3/image_features-1.pickle\n",
      "INFO:tensorflow:loading output/im2txt/feature_extraction_inception_v3/image_features-7.pickle\n",
      "(8091, 2048)\n",
      "(8091,)\n",
      "INFO:tensorflow:img_feature_dim: 2048\n",
      "INFO:tensorflow:caption_data_size: 8091\n",
      "array([[3.82126153e-01, 2.88703531e-01, 3.58352751e-01, ...,\n",
      "        2.95435369e-01, 2.69791968e-02, 1.02439411e-01],\n",
      "       [1.85586944e-01, 1.76685303e-01, 3.02964598e-01, ...,\n",
      "        5.37237883e-01, 1.28825650e-01, 2.38910869e-01],\n",
      "       [8.86647105e-01, 2.35739917e-01, 8.85686427e-02, ...,\n",
      "        1.44694597e-01, 1.01948154e+00, 1.49599552e-01],\n",
      "       [5.97755499e-02, 2.29833633e-01, 3.28001142e-01, ...,\n",
      "        2.44027242e-01, 1.52769499e-04, 9.40782577e-03],\n",
      "       [1.48593932e-01, 2.28245825e-01, 7.54655004e-02, ...,\n",
      "        2.22261831e-01, 5.08701131e-02, 1.09264955e-01]], dtype=float32)\n",
      "array([[   3,    9,    7,   33,   36,   73,    9,  102,  155,  366],\n",
      "       [   1,    9,   35,    0,    2,    2,    2,    2,    2,    2],\n",
      "       [   3, 3029,    9,   76,  220,  113,    0,    4,  175,   24],\n",
      "       [   3,   20,    4,    1,  595,   18,  249,    8,   15,   98],\n",
      "       [  13,   27,   32, 3157,    4,    5,   43,    2,    2,    2]])\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])\n",
      "array(['3415809168_af9dabdba5.jpg', '2297471897_3419605c16.jpg',\n",
      "       '278007543_99f5a91a3e.jpg', '289860281_94d699f36d.jpg',\n",
      "       '3292277400_f95cdd13d1.jpg'], dtype='<U25')\n"
     ]
    }
   ],
   "source": [
    "class ImageCaptionData(object):\n",
    "    def __init__(self,\n",
    "                 img_name_to_token_ids,\n",
    "                 img_feature_dir,\n",
    "                 num_timesteps,\n",
    "                 vocab,\n",
    "                 deterministic = False):\n",
    "        self._vocab = vocab\n",
    "        self._all_img_feature_filepaths = []\n",
    "        for filename in gfile.ListDirectory(img_feature_dir):\n",
    "            self._all_img_feature_filepaths.append(os.path.join(img_feature_dir, filename))\n",
    "        pprint.pprint(self._all_img_feature_filepaths)\n",
    "\n",
    "        self._img_name_to_token_ids = img_name_to_token_ids\n",
    "        self._num_timesteps = num_timesteps\n",
    "        self._indicator = 0\n",
    "        self._deterministic = deterministic\n",
    "        self._img_feature_filenames = []\n",
    "        self._img_feature_data = []\n",
    "        self._load_img_feature_pickle()\n",
    "        if not self._deterministic:\n",
    "            self._random_shuffle()\n",
    "\n",
    "\n",
    "    def _load_img_feature_pickle(self):\n",
    "        for filepath in self._all_img_feature_filepaths:\n",
    "            logging.info(\"loading %s\" % filepath)\n",
    "            with gfile.GFile(filepath, 'rb') as f:\n",
    "                filenames, features = pickle.load(f, encoding='iso-8859-1')\n",
    "                self._img_feature_filenames += filenames\n",
    "                self._img_feature_data.append(features)\n",
    "        # [(1000, 1, 1, 2048), (1000, 1, 1, 2048)] -> (2000, 1, 1, 2048)       \n",
    "        self._img_feature_data = np.vstack(self._img_feature_data)\n",
    "        origin_shape = self._img_feature_data.shape\n",
    "        self._img_feature_data = np.reshape(\n",
    "            self._img_feature_data, (origin_shape[0], origin_shape[3]))\n",
    "        self._img_feature_filenames = np.asarray(self._img_feature_filenames)\n",
    "#         print(self._img_feature_data.shape)\n",
    "#         print(self._img_feature_filenames.shape)\n",
    "        if not self._deterministic:\n",
    "            self._random_shuffle()\n",
    "\n",
    "\n",
    "    def size(self):\n",
    "        return len(self._img_feature_filenames)\n",
    "\n",
    "    def img_feature_size(self):\n",
    "        return self._img_feature_data.shape[1]\n",
    "\n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(self.size())\n",
    "        self._img_feature_filenames = self._img_feature_filenames[p]\n",
    "        self._img_feature_data = self._img_feature_data[p]\n",
    "\n",
    "    def _img_desc(self, filenames):\n",
    "        batch_sentence_ids = []\n",
    "        batch_weights = []\n",
    "        for filename in filenames:\n",
    "            token_ids_set = self._img_name_to_token_ids[filename]\n",
    "            # chosen_token_ids = random.choice(token_ids_set)\n",
    "            chosen_token_ids = token_ids_set[0]\n",
    "            chosen_token_length = len(chosen_token_ids)\n",
    "\n",
    "            weight = [1 for i in range(chosen_token_length)]\n",
    "            if chosen_token_length >= self._num_timesteps:\n",
    "                chosen_token_ids = chosen_token_ids[0:self._num_timesteps]\n",
    "                weight = weight[0:self._num_timesteps]\n",
    "            else:\n",
    "                remaining_length = self._num_timesteps - chosen_token_length\n",
    "                chosen_token_ids += [self._vocab.eos for i in range(remaining_length)]\n",
    "                weight += [0 for i in range(remaining_length)]\n",
    "            batch_sentence_ids.append(chosen_token_ids)\n",
    "            batch_weights.append(weight)\n",
    "        batch_sentence_ids = np.asarray(batch_sentence_ids)\n",
    "        batch_weights = np.asarray(batch_weights)\n",
    "        return batch_sentence_ids, batch_weights\n",
    "\n",
    "    def next(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self.size():\n",
    "            if not self._deterministic:\n",
    "                self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = self._indicator + batch_size\n",
    "        assert end_indicator <= self.size()\n",
    "\n",
    "        batch_img_features = self._img_feature_data[self._indicator: end_indicator]\n",
    "        batch_img_names = self._img_feature_filenames[self._indicator: end_indicator]\n",
    "        batch_sentence_ids, batch_weights = self._img_desc(batch_img_names)\n",
    "\n",
    "        self._indicator = end_indicator\n",
    "        return batch_img_features, batch_sentence_ids, batch_weights, batch_img_names\n",
    "\n",
    "\n",
    "caption_data = ImageCaptionData(img_name_to_token_ids, input_img_feature_dir, hps.num_timesteps, vocab)\n",
    "img_feature_dim = caption_data.img_feature_size()\n",
    "caption_data_size = caption_data.size()\n",
    "logging.info(\"img_feature_dim: %d\" % img_feature_dim)\n",
    "logging.info(\"caption_data_size: %d\" % caption_data_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 打印5个训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0.38043135, 0.17472209, 0.5443178 , ..., 0.00877173, 0.29356447,\n",
      "        0.09353122],\n",
      "       [0.4395943 , 0.31850204, 0.4975333 , ..., 0.26805463, 0.13659976,\n",
      "        1.560525  ],\n",
      "       [0.4324614 , 1.0902749 , 0.24328329, ..., 0.10131855, 0.09588131,\n",
      "        0.10705913],\n",
      "       [0.05053167, 0.20598187, 0.17265522, ..., 0.22788467, 0.27237055,\n",
      "        0.3056258 ],\n",
      "       [0.18016618, 0.19748989, 0.21104841, ..., 0.57722074, 0.45187607,\n",
      "        0.65886664]], dtype=float32)\n",
      "array([[   3,  796,   80,    4,   49,   12,    5,  501,   24,    1],\n",
      "       [   3,   11,    4,   26,  163,   46,    4,  161,   12,    1],\n",
      "       [   3,   20,    7,   79,   60,   82,   24,  306,    2,    2],\n",
      "       [   3,  520,  579,   24,    1,  616, 1474,    2,    2,    2],\n",
      "       [   3,   11,    4,  605,  438,  163,  128,    1,   14,    0]])\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "array(['3138504165_c7ae396294.jpg', '1414820925_3504c394e1.jpg',\n",
      "       '3243094580_ccd01679f5.jpg', '1141739219_2c47195e4c.jpg',\n",
      "       '3255017708_2b02bfcdcf.jpg'], dtype='<U25')\n"
     ]
    }
   ],
   "source": [
    "batch_img_features, batch_sentence_ids, batch_weights, batch_img_names = caption_data.next(5)\n",
    "pprint.pprint(batch_img_features)\n",
    "pprint.pprint(batch_sentence_ids)\n",
    "pprint.pprint(batch_weights)\n",
    "pprint.pprint(batch_img_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-27-4b7f5eaa631e>:4: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-27-4b7f5eaa631e>:49: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-27-4b7f5eaa631e>:55: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "INFO:tensorflow:variable name: embedding/embeddings:0\n",
      "INFO:tensorflow:variable name: image_feature_embed/dense/kernel:0\n",
      "INFO:tensorflow:variable name: image_feature_embed/dense/bias:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: fc/fc1/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc1/bias:0\n",
      "INFO:tensorflow:variable name: fc/logits/kernel:0\n",
      "INFO:tensorflow:variable name: fc/logits/bias:0\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Summary name embedding/embeddings:0_grad is illegal; using embedding/embeddings_0_grad instead.\n",
      "INFO:tensorflow:Summary name image_feature_embed/dense/kernel:0_grad is illegal; using image_feature_embed/dense/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name image_feature_embed/dense/bias:0_grad is illegal; using image_feature_embed/dense/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0_grad is illegal; using lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0_grad is illegal; using lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0_grad is illegal; using lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0_grad is illegal; using lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/fc1/kernel:0_grad is illegal; using fc/fc1/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/fc1/bias:0_grad is illegal; using fc/fc1/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/logits/kernel:0_grad is illegal; using fc/logits/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/logits/bias:0_grad is illegal; using fc/logits/bias_0_grad instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "def create_rnn_cell(hidden_dim, cell_type):\n",
    "    if cell_type == 'lstm':\n",
    "        return tf.contrib.rnn.BasicLSTMCell(hidden_dim, state_is_tuple=True)\n",
    "    elif cell_type == 'gru':\n",
    "        return tf.contrib.rnn.GRUCell(hidden_dim)\n",
    "    else:\n",
    "        raise Exception(\"%s has not been supported\" % cell_type)\n",
    "\n",
    "def dropout(cell, keep_prob):\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "def get_train_model(hps, vocab_size, img_feature_dim):\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "\n",
    "    img_feature  = tf.placeholder(tf.float32, (batch_size, img_feature_dim))\n",
    "    sentence = tf.placeholder(tf.int32, (batch_size, num_timesteps))\n",
    "    mask = tf.placeholder(tf.float32, (batch_size, num_timesteps))\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    global_step = tf.Variable(tf.zeros([], tf.int64), name='global_step', trainable=False)\n",
    "\n",
    "    # Sets up the embedding layer.\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    with tf.variable_scope('embedding', initializer=embedding_initializer):\n",
    "        embeddings = tf.get_variable(\n",
    "            'embeddings',\n",
    "            [vocab_size, hps.num_embedding_nodes],\n",
    "            tf.float32)\n",
    "        embed_token_ids = tf.nn.embedding_lookup(embeddings, sentence[:, 0:num_timesteps-1])\n",
    "\n",
    "    img_feature_embed_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('image_feature_embed', initializer=img_feature_embed_init):\n",
    "        embed_img = tf.layers.dense(img_feature, hps.num_embedding_nodes)\n",
    "        embed_img = tf.expand_dims(embed_img, 1)\n",
    "        embed_inputs = tf.concat([embed_img, embed_token_ids], axis=1)\n",
    "\n",
    "    # Sets up LSTM network.\n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_nodes + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    lstm_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    with tf.variable_scope('lstm_nn', initializer=lstm_init):\n",
    "        cells = []\n",
    "        for i in range(hps.num_lstm_layers):\n",
    "            cell = create_rnn_cell(hps.num_lstm_nodes[i], hps.cell_type)\n",
    "            cell = dropout(cell, keep_prob)\n",
    "            cells.append(cell)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "        initial_state = cell.zero_state(hps.batch_size, tf.float32)\n",
    "        # rnn_outputs: [batch_size, num_timesteps, hps.num_lstm_node[-1]]\n",
    "        rnn_outputs, _ = tf.nn.dynamic_rnn(cell,\n",
    "                                           embed_inputs,\n",
    "                                           initial_state=initial_state)\n",
    "\n",
    "    # Sets up the fully-connected layer.\n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('fc', initializer=fc_init):\n",
    "        rnn_outputs_2d = tf.reshape(rnn_outputs, [-1, hps.num_lstm_nodes[-1]])\n",
    "        fc1 = tf.layers.dense(rnn_outputs_2d, hps.num_fc_nodes, name='fc1')\n",
    "        fc1_dropout = tf.contrib.layers.dropout(fc1, keep_prob)\n",
    "        fc1_dropout = tf.nn.relu(fc1_dropout)\n",
    "        logits = tf.layers.dense(fc1_dropout, vocab_size, name='logits')\n",
    "\n",
    "    with tf.variable_scope('loss'):\n",
    "        sentence_flatten = tf.reshape(sentence, [-1])\n",
    "        mask_flatten = tf.reshape(mask, [-1])\n",
    "        mask_sum = tf.reduce_sum(mask_flatten)\n",
    "        softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=sentence_flatten)\n",
    "        weighted_softmax_loss = tf.multiply(softmax_loss,\n",
    "                                            tf.cast(mask_flatten, tf.float32))\n",
    "        prediction = tf.argmax(logits, 1, output_type = tf.int32)\n",
    "        correct_prediction = tf.equal(prediction, sentence_flatten)\n",
    "        correct_prediction_with_mask = tf.multiply(\n",
    "            tf.cast(correct_prediction, tf.float32),\n",
    "            mask_flatten)\n",
    "        accuracy = tf.reduce_sum(correct_prediction_with_mask) / mask_sum\n",
    "        loss = tf.reduce_sum(weighted_softmax_loss) / mask_sum\n",
    "        tf.summary.scalar('loss', loss)\n",
    "\n",
    "    with tf.variable_scope('train_op'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            logging.info(\"variable name: %s\" % (var.name))\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(loss, tvars), hps.clip_lstm_grads)\n",
    "        for grad, var in zip(grads, tvars):\n",
    "            tf.summary.histogram('%s_grad' % (var.name), grad)\n",
    "        optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=global_step)\n",
    "\n",
    "    return ((img_feature, sentence, mask, keep_prob),\n",
    "            (loss, accuracy, train_op),\n",
    "            global_step)\n",
    "\n",
    "placeholders, metrics, global_step = get_train_model(hps, vocab_size, img_feature_dim)\n",
    "img_feature, sentence, mask, keep_prob = placeholders\n",
    "loss, accuracy, train_op = metrics\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(max_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Step:   500, loss: 4.079, accuracy: 0.297\n",
      "INFO:tensorflow:Step:  1000, loss: 3.826, accuracy: 0.295\n",
      "INFO:tensorflow:Step:  1500, loss: 3.755, accuracy: 0.317\n",
      "INFO:tensorflow:Step:  2000, loss: 3.437, accuracy: 0.341\n",
      "INFO:tensorflow:Step: 2000, image caption model saved\n",
      "INFO:tensorflow:Step:  2500, loss: 3.314, accuracy: 0.364\n",
      "INFO:tensorflow:Step:  3000, loss: 3.003, accuracy: 0.402\n",
      "INFO:tensorflow:Step:  3500, loss: 3.080, accuracy: 0.400\n",
      "INFO:tensorflow:Step:  4000, loss: 3.034, accuracy: 0.398\n",
      "INFO:tensorflow:Step: 4000, image caption model saved\n",
      "INFO:tensorflow:Step:  4500, loss: 2.993, accuracy: 0.391\n",
      "INFO:tensorflow:Step:  5000, loss: 2.999, accuracy: 0.372\n",
      "INFO:tensorflow:Step:  5500, loss: 2.831, accuracy: 0.427\n",
      "INFO:tensorflow:Step:  6000, loss: 2.978, accuracy: 0.386\n",
      "INFO:tensorflow:Step: 6000, image caption model saved\n",
      "INFO:tensorflow:Step:  6500, loss: 3.080, accuracy: 0.394\n",
      "INFO:tensorflow:Step:  7000, loss: 2.778, accuracy: 0.402\n",
      "INFO:tensorflow:Step:  7500, loss: 2.698, accuracy: 0.433\n",
      "INFO:tensorflow:Step:  8000, loss: 2.681, accuracy: 0.441\n",
      "INFO:tensorflow:Step: 8000, image caption model saved\n",
      "INFO:tensorflow:Step:  8500, loss: 2.604, accuracy: 0.445\n",
      "INFO:tensorflow:Step:  9000, loss: 2.608, accuracy: 0.405\n",
      "INFO:tensorflow:Step:  9500, loss: 2.516, accuracy: 0.452\n",
      "INFO:tensorflow:Step: 10000, loss: 2.600, accuracy: 0.411\n",
      "INFO:tensorflow:Step: 10000, image caption model saved\n"
     ]
    }
   ],
   "source": [
    "training_steps = 10000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    writer = tf.summary.FileWriter(output_dir, sess.graph)\n",
    "    for i in range(training_steps):\n",
    "        batch_img_features, batch_sentence_ids, batch_weights, _ = caption_data.next(hps.batch_size)\n",
    "        input_vals = (batch_img_features, batch_sentence_ids, batch_weights, hps.keep_prob)\n",
    "        \n",
    "        feed_dict = dict(zip(placeholders, input_vals))\n",
    "        fetches = [global_step, loss, accuracy, train_op]\n",
    "        \n",
    "        should_log = (i + 1) % hps.log_frequent == 0\n",
    "        should_save = (i + 1) % hps.save_frequent == 0\n",
    "        if should_log:\n",
    "            fetches += [summary_op]\n",
    "        outputs = sess.run(fetches, feed_dict)\n",
    "        global_step_val, loss_val, accuracy_val = outputs[0:3]\n",
    "        if should_log:\n",
    "            summary_str = outputs[4]\n",
    "            writer.add_summary(summary_str, global_step_val)\n",
    "            logging.info('Step: %5d, loss: %3.3f, accuracy: %3.3f'\n",
    "                         % (global_step_val, loss_val, accuracy_val))\n",
    "        if should_save:\n",
    "            logging.info(\"Step: %d, image caption model saved\" % (global_step_val))\n",
    "            saver.save(sess, os.path.join(output_dir, \"image_caption\"), global_step=global_step_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:amazonei_tensorflow_p36]",
   "language": "python",
   "name": "conda-env-amazonei_tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
